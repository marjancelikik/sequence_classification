{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (3.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: xxhash in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (3.11.8)\n",
      "Requirement already satisfied: pandas in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: filelock in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: packaging in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (4.46.3)\n",
      "Requirement already satisfied: requests in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: filelock in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (2.5.1)\n",
      "Requirement already satisfied: fsspec in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: filelock in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: networkx in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pydantic in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (2.10.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pydantic) (2.27.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: no matches found: transformers[torch]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (1.1.1)\n",
      "Requirement already satisfied: pyyaml in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: psutil in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (0.26.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: filelock in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: requests in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (0.26.3)\n",
      "Requirement already satisfied: dill in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: multiprocess in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: packaging in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: xxhash in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: pandas in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (2024.9.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (3.11.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: filelock in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.0.2)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.3/249.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.55.0-cp39-cp39-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pillow>=8 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib) (11.0.0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.55.0 importlib-resources-6.4.5 kiwisolver-1.4.7 matplotlib-3.9.2 pyparsing-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.15.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install pydantic\n",
    "%pip install transformers[torch]\n",
    "%pip install accelerate -U\n",
    "%pip install scikit-learn\n",
    "%pip install evaluate\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the movie script, and save it in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script basic stats:\n",
      "{\n",
      "  \"total_words\": 21849,\n",
      "  \"total_dialogues\": 943,\n",
      "  \"total_words_in_dialogues\": 9457,\n",
      "  \"total_characters\": 83,\n",
      "  \"total_scenes\": 281\n",
      "}\n",
      "Class counts (character speeches):\n",
      "KORBEN with class id 41: 244 (0.25874867444326616)\n",
      "CORNELIUS with class id 21: 99 (0.10498409331919406)\n",
      "ZORG with class id 82: 68 (0.07211028632025451)\n",
      "LOC RHOD with class id 44: 52 (0.05514316012725345)\n",
      "PRESIDENT with class id 59: 48 (0.05090137857900318)\n",
      "LEELOO with class id 43: 48 (0.05090137857900318)\n",
      "MUNRO with class id 55: 36 (0.03817603393425239)\n",
      "PROFESSOR with class id 65: 24 (0.02545068928950159)\n",
      "MACTILBURGH with class id 45: 16 (0.016967126193001062)\n",
      "STAEDERT with class id 72: 15 (0.015906680805938492)\n",
      "CAPTAIN with class id 8: 15 (0.015906680805938492)\n",
      "RIGHT ARM with class id 66: 15 (0.015906680805938492)\n",
      "PRIEST with class id 61: 14 (0.014846235418875928)\n",
      "FINGER (V.O.) with class id 29: 13 (0.013785790031813362)\n",
      "DAVID with class id 22: 12 (0.012725344644750796)\n",
      "VOICE (O.S.) with class id 80: 11 (0.01166489925768823)\n",
      "GIRL with class id 34: 10 (0.010604453870625663)\n",
      "DIVA with class id 24: 10 (0.010604453870625663)\n",
      "SCIENTIST with class id 68: 9 (0.009544008483563097)\n",
      "VOICE with class id 79: 9 (0.009544008483563097)\n",
      "MOTHER (V.O.) with class id 53: 9 (0.009544008483563097)\n",
      "COP with class id 15: 8 (0.008483563096500531)\n",
      "KOMMANDER with class id 40: 7 (0.007423117709437964)\n",
      "AKANIT with class id 1: 7 (0.007423117709437964)\n",
      "CHECK-IN ATTENDANT with class id 10: 7 (0.007423117709437964)\n",
      "MUGGER with class id 54: 6 (0.006362672322375398)\n",
      "THAI with class id 77: 6 (0.006362672322375398)\n",
      "COP 1 with class id 17: 6 (0.006362672322375398)\n",
      "STEWARDESS with class id 73: 6 (0.006362672322375398)\n",
      "PILOT with class id 57: 6 (0.006362672322375398)\n",
      "SHADOW with class id 71: 6 (0.006362672322375398)\n",
      "NEIGHBOR with class id 56: 5 (0.005302226935312832)\n",
      "FOG with class id 31: 5 (0.005302226935312832)\n",
      "TECHNICIAN with class id 76: 4 (0.0042417815482502655)\n",
      "AIDE with class id 0: 4 (0.0042417815482502655)\n",
      "AKNOT with class id 2: 4 (0.0042417815482502655)\n",
      "COPILOT with class id 20: 4 (0.0042417815482502655)\n",
      "SECRETARY (O.S.) with class id 69: 4 (0.0042417815482502655)\n",
      "CHIEF OF POLICE with class id 12: 4 (0.0042417815482502655)\n",
      "BILLY with class id 7: 3 (0.003181336161187699)\n",
      "HEAD SCIENTIST with class id 37: 3 (0.003181336161187699)\n",
      "COMMANDER with class id 14: 3 (0.003181336161187699)\n",
      "MANGALORE with class id 47: 3 (0.003181336161187699)\n",
      "PRESIDENT (O.S.) with class id 60: 2 (0.0021208907741251328)\n",
      "HEAD CHEMISTS with class id 36: 2 (0.0021208907741251328)\n",
      "FIRST OFFICER with class id 30: 2 (0.0021208907741251328)\n",
      "GENERAL MUNRO with class id 33: 2 (0.0021208907741251328)\n",
      "ASSISTANT with class id 3: 2 (0.0021208907741251328)\n",
      "CHIEF with class id 11: 2 (0.0021208907741251328)\n",
      "VOCODER (O.S.) with class id 78: 2 (0.0021208907741251328)\n",
      "STEWARDESS 2 with class id 75: 2 (0.0021208907741251328)\n",
      "MECHANIC with class id 50: 2 (0.0021208907741251328)\n",
      "GROUND CREW MEMBER with class id 35: 2 (0.0021208907741251328)\n",
      "HOSTESS with class id 39: 2 (0.0021208907741251328)\n",
      "MOTHER (O.S.) with class id 51: 2 (0.0021208907741251328)\n",
      "DIVA'S ASSISTANT with class id 25: 2 (0.0021208907741251328)\n",
      "BABY RAY with class id 4: 2 (0.0021208907741251328)\n",
      "MANAGER with class id 46: 2 (0.0021208907741251328)\n",
      "PRIEST (V.O.) with class id 63: 1 (0.0010604453870625664)\n",
      "CLERK with class id 13: 1 (0.0010604453870625664)\n",
      "PRIEST (O.S.) with class id 62: 1 (0.0010604453870625664)\n",
      "GENERAL with class id 32: 1 (0.0010604453870625664)\n",
      "DOCTOR with class id 26: 1 (0.0010604453870625664)\n",
      "VOICE / KORBEN with class id 81: 1 (0.0010604453870625664)\n",
      "COP 3 with class id 19: 1 (0.0010604453870625664)\n",
      "COP (O.S.) with class id 16: 1 (0.0010604453870625664)\n",
      "MOTHER (V.O) with class id 52: 1 (0.0010604453870625664)\n",
      "SECURITY GUARD with class id 70: 1 (0.0010604453870625664)\n",
      "ROBOT with class id 67: 1 (0.0010604453870625664)\n",
      "DAVID (V.O.) with class id 23: 1 (0.0010604453870625664)\n",
      "STEWARDESS 1 with class id 74: 1 (0.0010604453870625664)\n",
      "HOST with class id 38: 1 (0.0010604453870625664)\n",
      "BEAT-UP COP with class id 5: 1 (0.0010604453870625664)\n",
      "CAPTAIN (V.O.) with class id 9: 1 (0.0010604453870625664)\n",
      "MANGALORE 1 with class id 48: 1 (0.0010604453870625664)\n",
      "MANGALORE 2 with class id 49: 1 (0.0010604453870625664)\n",
      "PRINCESS AACHEN with class id 64: 1 (0.0010604453870625664)\n",
      "EMPEROR JAPHET with class id 28: 1 (0.0010604453870625664)\n",
      "EMPEROR with class id 27: 1 (0.0010604453870625664)\n",
      "COP 2 with class id 18: 1 (0.0010604453870625664)\n",
      "POLICEMAN with class id 58: 1 (0.0010604453870625664)\n",
      "BEEPER (O.S.) with class id 6: 1 (0.0010604453870625664)\n",
      "KORBEN (O.S.) with class id 42: 1 (0.0010604453870625664)\n",
      "{\"text\": \"(deciphering) \\\"..when the three planets are in eclipse..\\\" \", \"label\": 65}\n",
      "{\"text\": \"\\\"..the black hole like a door is open... evil comes ... sowing terror and chaos...\\\" see?  the snake, billy.  the ultimate evil ... make sure you get the snake! \", \"label\": 65}\n",
      "{\"text\": \"and when is this door opening snake act supposed to occur? \", \"label\": 7}\n",
      "{\"text\": \"..if this is the five..and this the thousand.. \", \"label\": 65}\n",
      "{\"text\": \"every five thousand years.. \", \"label\": 65}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from  collections import Counter\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data.parse_movie_script import MovieScriptParser\n",
    "\n",
    "# Parse the dataset in structured format and output a json compatible to parse with Hugging face datasets library.\n",
    "# The output will be a json file with the following structure:\n",
    "# { \"text\": ..., \"label\": }\n",
    "parsed_object = MovieScriptParser.from_text_file(\"../data/raw/5thelement.txt\")\n",
    "\n",
    "# Print the stats for the parsed script\n",
    "print(\"Script basic stats:\")\n",
    "print(json.dumps(parsed_object.stats, indent=2))\n",
    "\n",
    "# Print counts per class\n",
    "class_map = parsed_object.get_vocabulary_to_label_mapping()\n",
    "class_counter = Counter()\n",
    "for scene in parsed_object.scenes:\n",
    "    for entry in scene.entries:\n",
    "        if entry.dialogue is not None:\n",
    "            class_counter[entry.dialogue.character] += 1\n",
    "\n",
    "print(\"Class counts (character speeches):\")\n",
    "for key, value in class_counter.most_common():\n",
    "    print(f\"{key} with class id {class_map[key]}: {class_counter[key]} ({class_counter[key] / parsed_object.stats['total_dialogues']})\")\n",
    "\n",
    "# Save the parsed script in json format\n",
    "parsed_object.save_character_dialogue_dataset_in_json_format(\"../data/parsed/5thelement.json\")\n",
    "\n",
    "# Print a few lines of the saved file to see the format\n",
    "with open(\"../data/parsed/5thelement.json\", \"r\") as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a basic traind and test split of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 754, Test Size: 189\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def split_dataset_from_json(json_file: str, test_size: float = 0.2):\n",
    "    \"\"\"\n",
    "    Load a dataset from a json file and split it into train and test sets by using datasets library from Hugging Face.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): The path to the json file containing the dataset.\n",
    "        test_size (float): The proportion of the dataset to include in the test split.\n",
    "    \"\"\"\n",
    "    # Load dataset from local json file \n",
    "    dataset = load_dataset(\"json\", data_files=json_file)\n",
    "\n",
    "    # Split the dataset into a simple train, validation and test sets\n",
    "    train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "    train_dataset = train_test_split['train']\n",
    "    test_dataset = train_test_split['test']\n",
    "\n",
    "    print(f\"Train Size: {len(train_dataset)}, Test Size: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = split_dataset_from_json(\"../data/parsed/5thelement.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a first exploratory Hugging face Transformer model with typical params and do basic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 189/189 [00:00<00:00, 8727.84 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Train Dataset:\n",
      "[[1, 1116, 768, 4, 1437, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1640, 282, 13533, 154, 43, 2649, 4, 1437, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[21, 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                               \n",
      " 50%|█████     | 24/48 [00:21<00:16,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.14119291305542, 'eval_accuracy': 0.24338624338624337, 'eval_f1': 0.009321175278622088, 'eval_runtime': 3.9445, 'eval_samples_per_second': 47.914, 'eval_steps_per_second': 6.084, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 30/48 [00:33<00:26,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2792, 'grad_norm': 4.887906074523926, 'learning_rate': 4.736842105263158e-06, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 48/48 [00:55<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.865752935409546, 'eval_accuracy': 0.24338624338624337, 'eval_f1': 0.009321175278622088, 'eval_runtime': 4.9899, 'eval_samples_per_second': 37.876, 'eval_steps_per_second': 4.81, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:58<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 58.6032, 'train_samples_per_second': 25.732, 'train_steps_per_second': 0.819, 'train_loss': 4.1466875076293945, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results on Test Set: {'eval_loss': 3.865752935409546, 'eval_accuracy': 0.24338624338624337, 'eval_f1': 0.009321175278622088, 'eval_runtime': 3.9302, 'eval_samples_per_second': 48.089, 'eval_steps_per_second': 6.107, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "### Train model\n",
    "model_name = \"microsoft/deberta-base\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add a padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use <EOS> as <PAD>\n",
    "\n",
    "# Tokenize the data\n",
    "def preprocess_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],  # raw text to be tokenized\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = examples[\"label\"]  # add the labels to the tokenized input\n",
    "    return tokenized\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Sanity check: print a few examples of the encoded dataset\n",
    "print(\"Encoded Train Dataset:\")\n",
    "print(encoded_train_dataset[\"input_ids\"][:2])\n",
    "print(encoded_train_dataset[\"attention_mask\"][:2])\n",
    "print(encoded_train_dataset[\"labels\"][:2])\n",
    "\n",
    "# Load the model\n",
    "num_labels = len(parsed_object.character_vocabulary)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Add a padding token if not present\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=30,\n",
    "    warmup_steps=10,  # learning rate will be gradually increased during the first 10 steps\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    metric_acc = evaluate.load(\"accuracy\")\n",
    "    metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "    accuracy = metric_acc.compute(predictions=predictions, references=labels)\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute confusion matrix and additional metrics per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# predictions in test set: 189\n",
      "Predictions on test set:\n",
      "[41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41]\n",
      "True labels:\n",
      "[41 34 22 45  0 21 41 80 56 66 82 41 41 65 76 41 41 82 82 59 44 55 41 55\n",
      " 21  1 41 73 66 71 59 61 82 21 72 68 41 59 55 64 21 66 21 43 41 59 24 76\n",
      " 41 55 41 41 41 41  7 77 41 21 59 41 21 41 41 44  4 43 43 82 59 43 21 82\n",
      " 80 55 68 82 41 79 21 41 43 21 82 59 41 73 44 41 41 40 41 29 61 21 41 80\n",
      " 33 39 41 56 55 21 72 29 43 66 65 41 68 41 21 24 41 53 82 59 72 21 21 45\n",
      "  8 24 21 44 29 68 43 82 17 41 43 44 65 17 34 77 41 70 43 65 44 41 41 65\n",
      " 21 53 21 41 55 33 41 12 40 43 41 10 21 41 43 59 55 41 43  8 41 15 41 41\n",
      " 55 22 20 61 43 43 35 21 41 44 44 21 45 41 59 41 41 44 55 58 65]\n",
      "Precision and recall per class:\n",
      "[{'Class': 41, 'Precision': 0.24338624338624337, 'Recall': 1.0}, {'Class': 0, 'Precision': 0, 'Recall': 0.0}, {'Class': 1, 'Precision': 0, 'Recall': 0.0}, {'Class': 2, 'Precision': 0, 'Recall': 0}, {'Class': 3, 'Precision': 0, 'Recall': 0}, {'Class': 4, 'Precision': 0, 'Recall': 0.0}, {'Class': 5, 'Precision': 0, 'Recall': 0}, {'Class': 6, 'Precision': 0, 'Recall': 0}, {'Class': 7, 'Precision': 0, 'Recall': 0.0}, {'Class': 8, 'Precision': 0, 'Recall': 0.0}, {'Class': 9, 'Precision': 0, 'Recall': 0}, {'Class': 10, 'Precision': 0, 'Recall': 0.0}, {'Class': 11, 'Precision': 0, 'Recall': 0}, {'Class': 12, 'Precision': 0, 'Recall': 0.0}, {'Class': 13, 'Precision': 0, 'Recall': 0}, {'Class': 14, 'Precision': 0, 'Recall': 0}, {'Class': 15, 'Precision': 0, 'Recall': 0.0}, {'Class': 16, 'Precision': 0, 'Recall': 0}, {'Class': 17, 'Precision': 0, 'Recall': 0.0}, {'Class': 18, 'Precision': 0, 'Recall': 0}, {'Class': 19, 'Precision': 0, 'Recall': 0}, {'Class': 20, 'Precision': 0, 'Recall': 0.0}, {'Class': 21, 'Precision': 0, 'Recall': 0.0}, {'Class': 22, 'Precision': 0, 'Recall': 0.0}, {'Class': 23, 'Precision': 0, 'Recall': 0}, {'Class': 24, 'Precision': 0, 'Recall': 0.0}, {'Class': 25, 'Precision': 0, 'Recall': 0}, {'Class': 26, 'Precision': 0, 'Recall': 0}, {'Class': 27, 'Precision': 0, 'Recall': 0}, {'Class': 28, 'Precision': 0, 'Recall': 0}, {'Class': 29, 'Precision': 0, 'Recall': 0.0}, {'Class': 30, 'Precision': 0, 'Recall': 0}, {'Class': 31, 'Precision': 0, 'Recall': 0}, {'Class': 32, 'Precision': 0, 'Recall': 0}, {'Class': 33, 'Precision': 0, 'Recall': 0.0}, {'Class': 34, 'Precision': 0, 'Recall': 0.0}, {'Class': 35, 'Precision': 0, 'Recall': 0.0}, {'Class': 36, 'Precision': 0, 'Recall': 0}, {'Class': 37, 'Precision': 0, 'Recall': 0}, {'Class': 38, 'Precision': 0, 'Recall': 0}, {'Class': 39, 'Precision': 0, 'Recall': 0.0}, {'Class': 40, 'Precision': 0, 'Recall': 0.0}, {'Class': 42, 'Precision': 0, 'Recall': 0}, {'Class': 43, 'Precision': 0, 'Recall': 0.0}, {'Class': 44, 'Precision': 0, 'Recall': 0.0}, {'Class': 45, 'Precision': 0, 'Recall': 0.0}, {'Class': 46, 'Precision': 0, 'Recall': 0}, {'Class': 47, 'Precision': 0, 'Recall': 0}, {'Class': 48, 'Precision': 0, 'Recall': 0}, {'Class': 49, 'Precision': 0, 'Recall': 0}, {'Class': 50, 'Precision': 0, 'Recall': 0}, {'Class': 51, 'Precision': 0, 'Recall': 0}, {'Class': 52, 'Precision': 0, 'Recall': 0}, {'Class': 53, 'Precision': 0, 'Recall': 0.0}, {'Class': 54, 'Precision': 0, 'Recall': 0}, {'Class': 55, 'Precision': 0, 'Recall': 0.0}, {'Class': 56, 'Precision': 0, 'Recall': 0.0}, {'Class': 57, 'Precision': 0, 'Recall': 0}, {'Class': 58, 'Precision': 0, 'Recall': 0.0}, {'Class': 59, 'Precision': 0, 'Recall': 0.0}, {'Class': 60, 'Precision': 0, 'Recall': 0}, {'Class': 61, 'Precision': 0, 'Recall': 0.0}, {'Class': 62, 'Precision': 0, 'Recall': 0}, {'Class': 63, 'Precision': 0, 'Recall': 0}, {'Class': 64, 'Precision': 0, 'Recall': 0.0}, {'Class': 65, 'Precision': 0, 'Recall': 0.0}, {'Class': 66, 'Precision': 0, 'Recall': 0.0}, {'Class': 67, 'Precision': 0, 'Recall': 0}, {'Class': 68, 'Precision': 0, 'Recall': 0.0}, {'Class': 69, 'Precision': 0, 'Recall': 0}, {'Class': 70, 'Precision': 0, 'Recall': 0.0}, {'Class': 71, 'Precision': 0, 'Recall': 0.0}, {'Class': 72, 'Precision': 0, 'Recall': 0.0}, {'Class': 73, 'Precision': 0, 'Recall': 0.0}, {'Class': 74, 'Precision': 0, 'Recall': 0}, {'Class': 75, 'Precision': 0, 'Recall': 0}, {'Class': 76, 'Precision': 0, 'Recall': 0.0}, {'Class': 77, 'Precision': 0, 'Recall': 0.0}, {'Class': 78, 'Precision': 0, 'Recall': 0}, {'Class': 79, 'Precision': 0, 'Recall': 0.0}, {'Class': 80, 'Precision': 0, 'Recall': 0.0}, {'Class': 81, 'Precision': 0, 'Recall': 0}, {'Class': 82, 'Precision': 0, 'Recall': 0.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions on test set to compute additional metrics\n",
    "def get_predictions_and_labels(test_dataset, trainer):\n",
    "    import numpy as np\n",
    "\n",
    "    # Predict on the evaluation dataset\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "\n",
    "    # Extract predictions and true labels\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    y_true = predictions.label_ids\n",
    "    return y_pred, y_true\n",
    "\n",
    "\n",
    "def compute_precision_recall_per_class(y_pred, y_true, num_labels):\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "\n",
    "    def compute_precision_recall(confusion_matrix):\n",
    "        \"\"\"\n",
    "        Compute precision and recall for each class from a confusion matrix.\n",
    "        \n",
    "        :param confusion_matrix: 2D array, where rows are actual classes\n",
    "                                and columns are predicted classes.\n",
    "        :return: Dictionary with precision and recall for each class.\n",
    "        \"\"\"\n",
    "        num_classes = confusion_matrix.shape[0]\n",
    "        metrics = []\n",
    "        \n",
    "        for i in range(num_classes):\n",
    "            # True Positives (diagonal element)\n",
    "            TP = confusion_matrix[i, i]\n",
    "            \n",
    "            # False Positives (sum of column i, excluding TP)\n",
    "            FP = np.sum(confusion_matrix[:, i]) - TP\n",
    "            \n",
    "            # False Negatives (sum of row i, excluding TP)\n",
    "            FN = np.sum(confusion_matrix[i, :]) - TP\n",
    "            \n",
    "            # Precision and Recall\n",
    "            precision = float(TP / (TP + FP)) if (TP + FP) > 0 else 0\n",
    "            recall = float(TP / (TP + FN)) if (TP + FN) > 0 else 0\n",
    "            \n",
    "            metrics.append({'Class': i, 'Precision': precision, 'Recall': recall})\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    print(\"# predictions in test set: {}\".format(len(y_pred)))\n",
    "    print(\"Predictions on test set:\")\n",
    "    print(y_pred)\n",
    "    print(\"True labels:\")\n",
    "    print(y_true)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(num_labels))\n",
    "\n",
    "    # Compute precision and recall per class\n",
    "    metrics_per_class = compute_precision_recall(cm)\n",
    "    metrics_per_class = sorted(metrics_per_class, key=lambda x: x['Recall'], reverse=True)\n",
    "    \n",
    "    print(\"Precision and recall per class:\")\n",
    "    print(metrics_per_class)\n",
    "\n",
    "y_pred, y_true = get_predictions_and_labels(encoded_test_dataset, trainer)\n",
    "compute_precision_recall_per_class(y_pred, y_true, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate on oversampled dataset (oversample each class proportionally to max class size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 754\n",
      "Oversampled training dataset size: 7691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7691/7691 [00:00<00:00, 15532.54 examples/s]\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                 \n",
      " 16%|█▌        | 104/648 [16:37<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3557, 'grad_norm': 3.7209956645965576, 'learning_rate': 9.576271186440679e-06, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [17:00<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1152, 'grad_norm': 3.9727783203125, 'learning_rate': 8.940677966101694e-06, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [17:23<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9421, 'grad_norm': 4.489749908447266, 'learning_rate': 8.305084745762712e-06, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [17:48<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7317, 'grad_norm': 5.299050807952881, 'learning_rate': 7.66949152542373e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [18:11<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5218, 'grad_norm': 4.993688106536865, 'learning_rate': 7.033898305084746e-06, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [18:37<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3312, 'grad_norm': 5.515802383422852, 'learning_rate': 6.398305084745763e-06, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [19:02<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1153, 'grad_norm': 5.577577590942383, 'learning_rate': 5.7627118644067805e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [19:25<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9275, 'grad_norm': 6.456362724304199, 'learning_rate': 5.127118644067796e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      "\n",
      " 16%|█▌        | 104/648 [19:36<07:33,  1.20it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.532827138900757, 'eval_accuracy': 0.2222222222222222, 'eval_f1': 0.24983444955427714, 'eval_runtime': 7.1677, 'eval_samples_per_second': 26.368, 'eval_steps_per_second': 3.348, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [20:17<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7448, 'grad_norm': 6.159979820251465, 'learning_rate': 4.491525423728814e-06, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [20:56<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6538, 'grad_norm': 5.831851482391357, 'learning_rate': 3.8559322033898315e-06, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [21:23<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.527, 'grad_norm': 5.727634429931641, 'learning_rate': 3.2203389830508473e-06, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [21:53<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4035, 'grad_norm': 6.117085933685303, 'learning_rate': 2.5847457627118645e-06, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [22:19<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2741, 'grad_norm': 6.15022611618042, 'learning_rate': 1.9491525423728816e-06, 'epoch': 1.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [22:49<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2912, 'grad_norm': 6.010318279266357, 'learning_rate': 1.3135593220338985e-06, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [23:15<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.204, 'grad_norm': 5.977491855621338, 'learning_rate': 6.779661016949153e-07, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 16%|█▌        | 104/648 [23:44<07:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2232, 'grad_norm': 6.491508960723877, 'learning_rate': 4.237288135593221e-08, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      "\n",
      " 16%|█▌        | 104/648 [23:58<07:33,  1.20it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.227496385574341, 'eval_accuracy': 0.26455026455026454, 'eval_f1': 0.31789557959770726, 'eval_runtime': 6.9893, 'eval_samples_per_second': 27.041, 'eval_steps_per_second': 3.434, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 482/482 [07:55<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 475.0976, 'train_samples_per_second': 32.377, 'train_steps_per_second': 1.015, 'train_loss': 3.0197754875752936, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:04<00:00,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# predictions in test set: 189\n",
      "Predictions on test set:\n",
      "[53 34 22 45  0 65 53 80 56 10 24 44 24 65 76 57 44 20 53 76 53 10 53 53\n",
      " 68  1 34 73 10 24 24 61 24 44 15 68 24 76 24 10  2 76 40 24 56 53 24 76\n",
      " 54 68 24 10 10 15 57 24 53 34 34  1 10 65 53 24 72 54 24 24 54 34 73 24\n",
      " 80 53 68 44  1 79 24 10 34 73 10 73 29 73 34 15 34 14 10 29 61 24 10 80\n",
      " 69 10 53 56 53 44 15 29 73 10 65 24 68 56 24 24 44 53 53 10 72 57 45 45\n",
      " 40 24 24 14 29 68 57 22 17 54 24 44 10 17 34 44 53 57 34 65 15 57 53 65\n",
      " 10 53 44 24 24 68 34 12 40 34 53 10 24 10 34 15 14 34 22  8 53 22 53 34\n",
      " 73 22 20 61 24 34 57 24 61 53 44 44 45 10 72 53 24 24 55 57 65]\n",
      "True labels:\n",
      "[41 34 22 45  0 21 41 80 56 66 82 41 41 65 76 41 41 82 82 59 44 55 41 55\n",
      " 21  1 41 73 66 71 59 61 82 21 72 68 41 59 55 64 21 66 21 43 41 59 24 76\n",
      " 41 55 41 41 41 41  7 77 41 21 59 41 21 41 41 44  4 43 43 82 59 43 21 82\n",
      " 80 55 68 82 41 79 21 41 43 21 82 59 41 73 44 41 41 40 41 29 61 21 41 80\n",
      " 33 39 41 56 55 21 72 29 43 66 65 41 68 41 21 24 41 53 82 59 72 21 21 45\n",
      "  8 24 21 44 29 68 43 82 17 41 43 44 65 17 34 77 41 70 43 65 44 41 41 65\n",
      " 21 53 21 41 55 33 41 12 40 43 41 10 21 41 43 59 55 41 43  8 41 15 41 41\n",
      " 55 22 20 61 43 43 35 21 41 44 44 21 45 41 59 41 41 44 55 58 65]\n",
      "Precision and recall per class:\n",
      "[{'Class': 0, 'Precision': 1.0, 'Recall': 1.0}, {'Class': 1, 'Precision': 0.3333333333333333, 'Recall': 1.0}, {'Class': 10, 'Precision': 0.05263157894736842, 'Recall': 1.0}, {'Class': 12, 'Precision': 1.0, 'Recall': 1.0}, {'Class': 17, 'Precision': 1.0, 'Recall': 1.0}, {'Class': 20, 'Precision': 0.5, 'Recall': 1.0}, {'Class': 22, 'Precision': 0.4, 'Recall': 1.0}, {'Class': 24, 'Precision': 0.1, 'Recall': 1.0}, {'Class': 29, 'Precision': 0.75, 'Recall': 1.0}, {'Class': 34, 'Precision': 0.125, 'Recall': 1.0}, {'Class': 45, 'Precision': 0.75, 'Recall': 1.0}, {'Class': 53, 'Precision': 0.09090909090909091, 'Recall': 1.0}, {'Class': 56, 'Precision': 0.5, 'Recall': 1.0}, {'Class': 61, 'Precision': 0.75, 'Recall': 1.0}, {'Class': 68, 'Precision': 0.5714285714285714, 'Recall': 1.0}, {'Class': 73, 'Precision': 0.2857142857142857, 'Recall': 1.0}, {'Class': 76, 'Precision': 0.4, 'Recall': 1.0}, {'Class': 79, 'Precision': 1.0, 'Recall': 1.0}, {'Class': 80, 'Precision': 1.0, 'Recall': 1.0}, {'Class': 65, 'Precision': 0.7142857142857143, 'Recall': 0.8333333333333334}, {'Class': 8, 'Precision': 1.0, 'Recall': 0.5}, {'Class': 40, 'Precision': 0.3333333333333333, 'Recall': 0.5}, {'Class': 72, 'Precision': 0.3333333333333333, 'Recall': 0.3333333333333333}, {'Class': 44, 'Precision': 0.18181818181818182, 'Recall': 0.2222222222222222}, {'Class': 55, 'Precision': 1.0, 'Recall': 0.1}, {'Class': 2, 'Precision': 0.0, 'Recall': 0}, {'Class': 3, 'Precision': 0, 'Recall': 0}, {'Class': 4, 'Precision': 0, 'Recall': 0.0}, {'Class': 5, 'Precision': 0, 'Recall': 0}, {'Class': 6, 'Precision': 0, 'Recall': 0}, {'Class': 7, 'Precision': 0, 'Recall': 0.0}, {'Class': 9, 'Precision': 0, 'Recall': 0}, {'Class': 11, 'Precision': 0, 'Recall': 0}, {'Class': 13, 'Precision': 0, 'Recall': 0}, {'Class': 14, 'Precision': 0.0, 'Recall': 0}, {'Class': 15, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 16, 'Precision': 0, 'Recall': 0}, {'Class': 18, 'Precision': 0, 'Recall': 0}, {'Class': 19, 'Precision': 0, 'Recall': 0}, {'Class': 21, 'Precision': 0, 'Recall': 0.0}, {'Class': 23, 'Precision': 0, 'Recall': 0}, {'Class': 25, 'Precision': 0, 'Recall': 0}, {'Class': 26, 'Precision': 0, 'Recall': 0}, {'Class': 27, 'Precision': 0, 'Recall': 0}, {'Class': 28, 'Precision': 0, 'Recall': 0}, {'Class': 30, 'Precision': 0, 'Recall': 0}, {'Class': 31, 'Precision': 0, 'Recall': 0}, {'Class': 32, 'Precision': 0, 'Recall': 0}, {'Class': 33, 'Precision': 0, 'Recall': 0.0}, {'Class': 35, 'Precision': 0, 'Recall': 0.0}, {'Class': 36, 'Precision': 0, 'Recall': 0}, {'Class': 37, 'Precision': 0, 'Recall': 0}, {'Class': 38, 'Precision': 0, 'Recall': 0}, {'Class': 39, 'Precision': 0, 'Recall': 0.0}, {'Class': 41, 'Precision': 0, 'Recall': 0.0}, {'Class': 42, 'Precision': 0, 'Recall': 0}, {'Class': 43, 'Precision': 0, 'Recall': 0.0}, {'Class': 46, 'Precision': 0, 'Recall': 0}, {'Class': 47, 'Precision': 0, 'Recall': 0}, {'Class': 48, 'Precision': 0, 'Recall': 0}, {'Class': 49, 'Precision': 0, 'Recall': 0}, {'Class': 50, 'Precision': 0, 'Recall': 0}, {'Class': 51, 'Precision': 0, 'Recall': 0}, {'Class': 52, 'Precision': 0, 'Recall': 0}, {'Class': 54, 'Precision': 0.0, 'Recall': 0}, {'Class': 57, 'Precision': 0.0, 'Recall': 0}, {'Class': 58, 'Precision': 0, 'Recall': 0.0}, {'Class': 59, 'Precision': 0, 'Recall': 0.0}, {'Class': 60, 'Precision': 0, 'Recall': 0}, {'Class': 62, 'Precision': 0, 'Recall': 0}, {'Class': 63, 'Precision': 0, 'Recall': 0}, {'Class': 64, 'Precision': 0, 'Recall': 0.0}, {'Class': 66, 'Precision': 0, 'Recall': 0.0}, {'Class': 67, 'Precision': 0, 'Recall': 0}, {'Class': 69, 'Precision': 0.0, 'Recall': 0}, {'Class': 70, 'Precision': 0, 'Recall': 0.0}, {'Class': 71, 'Precision': 0, 'Recall': 0.0}, {'Class': 74, 'Precision': 0, 'Recall': 0}, {'Class': 75, 'Precision': 0, 'Recall': 0}, {'Class': 77, 'Precision': 0, 'Recall': 0.0}, {'Class': 78, 'Precision': 0, 'Recall': 0}, {'Class': 81, 'Precision': 0, 'Recall': 0}, {'Class': 82, 'Precision': 0, 'Recall': 0.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "def oversample_dataset(dataset: Dataset, class_count_threshold=0):\n",
    "    \"\"\"\n",
    "    Oversamples the dataset to balance the class distribution. Ignore classes with count less or equal to class_count_threshold.\n",
    "    Args:\n",
    "        dataset (Dataset): A dataset object containing examples with a \"label\" field.\n",
    "        class_count_threshold (int): The minimum count of a class to be considered for oversampling.\n",
    "    Returns:\n",
    "        Dataset: A new dataset object with balanced class distribution by oversampling the minority classes.\n",
    "    \"\"\"\n",
    "    class_counts = Counter(dataset[\"label\"])\n",
    "\n",
    "    max_count = max(class_counts.values())\n",
    "    examples_by_class = {label: [] for label in class_counts}\n",
    "    \n",
    "    for example in dataset:\n",
    "        examples_by_class[example[\"label\"]].append(example)\n",
    "    \n",
    "    balanced_examples = []\n",
    "    for _, examples in examples_by_class.items():\n",
    "        if len(examples) > class_count_threshold:\n",
    "            balanced_examples.extend(random.choices(examples, k=max_count))\n",
    "        else:\n",
    "            balanced_examples.extend(examples)\n",
    "    \n",
    "    random.shuffle(balanced_examples)\n",
    "    return Dataset.from_list(balanced_examples)\n",
    "\n",
    "train_dataset_oversampled = oversample_dataset(train_dataset, class_count_threshold=2)\n",
    "\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Oversampled training dataset size:\", len(train_dataset_oversampled))\n",
    "\n",
    "encoded_train_dataset_oversampled = train_dataset_oversampled.map(preprocess_function, batched=True)\n",
    "\n",
    "# Reset model weights to previous state\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset_oversampled,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred, y_true = get_predictions_and_labels(encoded_test_dataset, trainer)\n",
    "compute_precision_recall_per_class(y_pred, y_true, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use weighted cross-entropy loss to simulate oversampling with less computational effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(2.2949985871139686), np.float64(1.9979123127758747), np.float64(2.2949985871139686), 1.0, 1.0, 1.0, 1.0, 1.0, np.float64(1.7698160790574151), 1.0, np.float64(1.9372562265956328), 1.0, np.float64(2.166679504533743), 1.0, np.float64(2.2949985871139686), np.float64(1.9372562265956328), 1.0, np.float64(1.9979123127758747), 1.0, 1.0, np.float64(2.166679504533743), np.float64(1.2023861220940364), np.float64(1.7698160790574151), 1.0, np.float64(1.8862040631540171), 1.0, 1.0, 1.0, 1.0, np.float64(1.8038759709114691), 1.0, np.float64(2.072109360404684), 1.0, 1.0, np.float64(1.8422907500573902), 1.0, 1.0, 1.0, 1.0, 1.0, np.float64(2.072109360404684), np.float64(1.0), 1.0, np.float64(1.3538071871541277), np.float64(1.3352548702231246), np.float64(1.7116619756873133), 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, np.float64(1.9979123127758747), np.float64(2.2949985871139686), np.float64(1.4578977774089315), np.float64(2.166679504533743), np.float64(2.072109360404684), 1.0, np.float64(1.4294753731708845), 1.0, np.float64(1.8038759709114691), 1.0, 1.0, 1.0, np.float64(1.5551160888671096), np.float64(1.7392836893032997), 1.0, np.float64(2.072109360404684), np.float64(2.2949985871139686), 1.0, np.float64(2.166679504533743), np.float64(1.8038759709114691), np.float64(1.9979123127758747), 1.0, 1.0, np.float64(2.166679504533743), np.float64(2.166679504533743), 1.0, np.float64(1.8422907500573902), np.float64(1.8038759709114691), 1.0, np.float64(1.2735960466682315)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/48 [10:48<8:27:43, 648.17s/it]\n",
      " 50%|█████     | 24/48 [00:34<00:20,  1.19it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                               \n",
      "\n",
      " 50%|█████     | 24/48 [00:38<00:20,  1.19it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.191098213195801, 'eval_accuracy': 0.24338624338624337, 'eval_f1': 0.009321175278622088, 'eval_runtime': 4.1902, 'eval_samples_per_second': 45.105, 'eval_steps_per_second': 5.728, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 30/48 [00:53<00:29,  1.64s/it]\n",
      " 62%|██████▎   | 30/48 [00:53<00:29,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3127, 'grad_norm': 4.599071979522705, 'learning_rate': 4.736842105263158e-06, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [01:15<00:00,  1.26s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                               \n",
      "\n",
      "100%|██████████| 48/48 [01:28<00:00,  1.26s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.955878973007202, 'eval_accuracy': 0.24338624338624337, 'eval_f1': 0.009321175278622088, 'eval_runtime': 7.8028, 'eval_samples_per_second': 24.222, 'eval_steps_per_second': 3.076, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 48/48 [01:40<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 100.5158, 'train_samples_per_second': 15.003, 'train_steps_per_second': 0.478, 'train_loss': 4.2025101979573565, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# predictions in test set: 189\n",
      "Predictions on test set:\n",
      "[41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41]\n",
      "True labels:\n",
      "[41 34 22 45  0 21 41 80 56 66 82 41 41 65 76 41 41 82 82 59 44 55 41 55\n",
      " 21  1 41 73 66 71 59 61 82 21 72 68 41 59 55 64 21 66 21 43 41 59 24 76\n",
      " 41 55 41 41 41 41  7 77 41 21 59 41 21 41 41 44  4 43 43 82 59 43 21 82\n",
      " 80 55 68 82 41 79 21 41 43 21 82 59 41 73 44 41 41 40 41 29 61 21 41 80\n",
      " 33 39 41 56 55 21 72 29 43 66 65 41 68 41 21 24 41 53 82 59 72 21 21 45\n",
      "  8 24 21 44 29 68 43 82 17 41 43 44 65 17 34 77 41 70 43 65 44 41 41 65\n",
      " 21 53 21 41 55 33 41 12 40 43 41 10 21 41 43 59 55 41 43  8 41 15 41 41\n",
      " 55 22 20 61 43 43 35 21 41 44 44 21 45 41 59 41 41 44 55 58 65]\n",
      "Precision and recall per class:\n",
      "[{'Class': 41, 'Precision': 0.24338624338624337, 'Recall': 1.0}, {'Class': 0, 'Precision': 0, 'Recall': 0.0}, {'Class': 1, 'Precision': 0, 'Recall': 0.0}, {'Class': 2, 'Precision': 0, 'Recall': 0}, {'Class': 3, 'Precision': 0, 'Recall': 0}, {'Class': 4, 'Precision': 0, 'Recall': 0.0}, {'Class': 5, 'Precision': 0, 'Recall': 0}, {'Class': 6, 'Precision': 0, 'Recall': 0}, {'Class': 7, 'Precision': 0, 'Recall': 0.0}, {'Class': 8, 'Precision': 0, 'Recall': 0.0}, {'Class': 9, 'Precision': 0, 'Recall': 0}, {'Class': 10, 'Precision': 0, 'Recall': 0.0}, {'Class': 11, 'Precision': 0, 'Recall': 0}, {'Class': 12, 'Precision': 0, 'Recall': 0.0}, {'Class': 13, 'Precision': 0, 'Recall': 0}, {'Class': 14, 'Precision': 0, 'Recall': 0}, {'Class': 15, 'Precision': 0, 'Recall': 0.0}, {'Class': 16, 'Precision': 0, 'Recall': 0}, {'Class': 17, 'Precision': 0, 'Recall': 0.0}, {'Class': 18, 'Precision': 0, 'Recall': 0}, {'Class': 19, 'Precision': 0, 'Recall': 0}, {'Class': 20, 'Precision': 0, 'Recall': 0.0}, {'Class': 21, 'Precision': 0, 'Recall': 0.0}, {'Class': 22, 'Precision': 0, 'Recall': 0.0}, {'Class': 23, 'Precision': 0, 'Recall': 0}, {'Class': 24, 'Precision': 0, 'Recall': 0.0}, {'Class': 25, 'Precision': 0, 'Recall': 0}, {'Class': 26, 'Precision': 0, 'Recall': 0}, {'Class': 27, 'Precision': 0, 'Recall': 0}, {'Class': 28, 'Precision': 0, 'Recall': 0}, {'Class': 29, 'Precision': 0, 'Recall': 0.0}, {'Class': 30, 'Precision': 0, 'Recall': 0}, {'Class': 31, 'Precision': 0, 'Recall': 0}, {'Class': 32, 'Precision': 0, 'Recall': 0}, {'Class': 33, 'Precision': 0, 'Recall': 0.0}, {'Class': 34, 'Precision': 0, 'Recall': 0.0}, {'Class': 35, 'Precision': 0, 'Recall': 0.0}, {'Class': 36, 'Precision': 0, 'Recall': 0}, {'Class': 37, 'Precision': 0, 'Recall': 0}, {'Class': 38, 'Precision': 0, 'Recall': 0}, {'Class': 39, 'Precision': 0, 'Recall': 0.0}, {'Class': 40, 'Precision': 0, 'Recall': 0.0}, {'Class': 42, 'Precision': 0, 'Recall': 0}, {'Class': 43, 'Precision': 0, 'Recall': 0.0}, {'Class': 44, 'Precision': 0, 'Recall': 0.0}, {'Class': 45, 'Precision': 0, 'Recall': 0.0}, {'Class': 46, 'Precision': 0, 'Recall': 0}, {'Class': 47, 'Precision': 0, 'Recall': 0}, {'Class': 48, 'Precision': 0, 'Recall': 0}, {'Class': 49, 'Precision': 0, 'Recall': 0}, {'Class': 50, 'Precision': 0, 'Recall': 0}, {'Class': 51, 'Precision': 0, 'Recall': 0}, {'Class': 52, 'Precision': 0, 'Recall': 0}, {'Class': 53, 'Precision': 0, 'Recall': 0.0}, {'Class': 54, 'Precision': 0, 'Recall': 0}, {'Class': 55, 'Precision': 0, 'Recall': 0.0}, {'Class': 56, 'Precision': 0, 'Recall': 0.0}, {'Class': 57, 'Precision': 0, 'Recall': 0}, {'Class': 58, 'Precision': 0, 'Recall': 0.0}, {'Class': 59, 'Precision': 0, 'Recall': 0.0}, {'Class': 60, 'Precision': 0, 'Recall': 0}, {'Class': 61, 'Precision': 0, 'Recall': 0.0}, {'Class': 62, 'Precision': 0, 'Recall': 0}, {'Class': 63, 'Precision': 0, 'Recall': 0}, {'Class': 64, 'Precision': 0, 'Recall': 0.0}, {'Class': 65, 'Precision': 0, 'Recall': 0.0}, {'Class': 66, 'Precision': 0, 'Recall': 0.0}, {'Class': 67, 'Precision': 0, 'Recall': 0}, {'Class': 68, 'Precision': 0, 'Recall': 0.0}, {'Class': 69, 'Precision': 0, 'Recall': 0}, {'Class': 70, 'Precision': 0, 'Recall': 0.0}, {'Class': 71, 'Precision': 0, 'Recall': 0.0}, {'Class': 72, 'Precision': 0, 'Recall': 0.0}, {'Class': 73, 'Precision': 0, 'Recall': 0.0}, {'Class': 74, 'Precision': 0, 'Recall': 0}, {'Class': 75, 'Precision': 0, 'Recall': 0}, {'Class': 76, 'Precision': 0, 'Recall': 0.0}, {'Class': 77, 'Precision': 0, 'Recall': 0.0}, {'Class': 78, 'Precision': 0, 'Recall': 0}, {'Class': 79, 'Precision': 0, 'Recall': 0.0}, {'Class': 80, 'Precision': 0, 'Recall': 0.0}, {'Class': 81, 'Precision': 0, 'Recall': 0}, {'Class': 82, 'Precision': 0, 'Recall': 0.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Reset model weights to previous state\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=num_labels\n",
    ")\n",
    "\n",
    "def compute_class_weights(dataset: Dataset, num_labels: int, min_count=0, top_k=None):\n",
    "    \"\"\"\n",
    "    Compute class weights for\n",
    "    Args:\n",
    "        dataset (Dataset): A dataset object containing examples with a \"label\" field.\n",
    "        num_labels (int): The number of classes in the dataset.\n",
    "        min_count (int): The minimum count of a class to be considered for computing class weights.\n",
    "    Returns:\n",
    "        dict: A dictionary with class weights for each class compared to the class with maximum count.\n",
    "    \"\"\"\n",
    "    class_counts = Counter(dataset[\"label\"])\n",
    "\n",
    "    if top_k is not None:\n",
    "        class_counts = dict(class_counts.most_common(top_k))\n",
    "\n",
    "    max_count = max(class_counts.values())\n",
    "    class_weights = {label: np.power(max_count / count, 0.5) for label, count in class_counts.items()}\n",
    "    class_weights_list = [1.0] * (num_labels)\n",
    "\n",
    "    for label, weight in class_weights.items():\n",
    "        if class_counts[label] > min_count:\n",
    "            class_weights_list[label] = weight\n",
    "            \n",
    "    return class_weights_list\n",
    "\n",
    "class_weights = compute_class_weights(train_dataset, num_labels=num_labels, min_count=2)\n",
    "print(class_weights)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Define custom compute_loss_func\n",
    "def compute_loss_func(outputs, labels, num_items_in_batch=None):\n",
    "    logits = outputs.logits\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    return loss\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    compute_loss_func=compute_loss_func,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred, y_true = get_predictions_and_labels(encoded_test_dataset, trainer)\n",
    "compute_precision_recall_per_class(y_pred, y_true, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sqlalchemy>=1.4.2\n",
      "  Downloading SQLAlchemy-2.0.36-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: numpy in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from optuna) (2.0.2)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tqdm in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from optuna) (6.0.2)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Mako\n",
      "  Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Installing collected packages: sqlalchemy, Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0 sqlalchemy-2.0.36\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "zsh:1: no matches found: ray[tune]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! pip install optuna\n",
    "! pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 565, Test Size: 189, Val Size: 189\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Split the dataset into train, validation and test sets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from local json file \n",
    "dataset = load_dataset(\"json\", data_files=\"./data/parsed/5thelement.json\")\n",
    "\n",
    "# Split the dataset into a simple train, validation and test sets\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.4)\n",
    "\n",
    "train_dataset = train_test_split['train']\n",
    "\n",
    "temp_split_dataset = train_test_split['test'].train_test_split(test_size=0.5)\n",
    "val_dataset = temp_split_dataset['train']\n",
    "test_dataset = temp_split_dataset['test']\n",
    "\n",
    "print(f\"Train Size: {len(train_dataset)}, Test Size: {len(test_dataset)}, Val Size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 7022.66 examples/s]\n",
      "Map: 100%|██████████| 189/189 [00:00<00:00, 8330.78 examples/s]\n",
      "Map: 100%|██████████| 189/189 [00:00<00:00, 8285.85 examples/s]\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2024-11-30 21:54:22,213] A new study created in memory with name: no-name-5662d24d-8aac-454c-971a-0841b2ea97d4\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                 \n",
      " 25%|██▌       | 189/756 [07:07<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4345, 'grad_norm': 4.090267658233643, 'learning_rate': 8.854597805338979e-06, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [07:18<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3045, 'grad_norm': 9.708539009094238, 'learning_rate': 6.482830536051752e-06, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [07:25<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7897, 'grad_norm': 10.829245567321777, 'learning_rate': 4.111063266764526e-06, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [07:33<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6095, 'grad_norm': 12.190399169921875, 'learning_rate': 1.7392959974772993e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      "\n",
      " 25%|██▌       | 189/756 [07:52<02:22,  3.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5981497764587402, 'eval_accuracy': 0.2222222222222222, 'eval_f1': 0.008658008658008658, 'eval_runtime': 7.1919, 'eval_samples_per_second': 26.28, 'eval_steps_per_second': 3.337, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 142/142 [00:57<00:00,  2.49it/s]\n",
      "[I 2024-11-30 21:55:21,377] Trial 0 finished with value: 0.23088023088023088 and parameters: {'learning_rate': 1.0435775984863796e-05, 'num_train_epochs': 1, 'seed': 7, 'per_device_train_batch_size': 4}. Best is trial 0 with value: 0.23088023088023088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 57.0963, 'train_samples_per_second': 9.896, 'train_steps_per_second': 2.487, 'train_loss': 3.969445242008693, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                 \n",
      " 25%|██▌       | 189/756 [08:09<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2952, 'grad_norm': 8.204130172729492, 'learning_rate': 8.606272816685223e-06, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [08:20<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7404, 'grad_norm': 10.060961723327637, 'learning_rate': 6.301021169358824e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\u001b[A                                             \n",
      "\n",
      " 25%|██▌       | 189/756 [08:29<02:22,  3.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.646695613861084, 'eval_accuracy': 0.2222222222222222, 'eval_f1': 0.008658008658008658, 'eval_runtime': 5.448, 'eval_samples_per_second': 34.691, 'eval_steps_per_second': 4.405, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [08:41<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6168, 'grad_norm': 7.278314113616943, 'learning_rate': 3.9957695220324245e-06, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [08:50<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4228, 'grad_norm': 7.428894519805908, 'learning_rate': 1.6905178747060257e-06, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      " 25%|██▌       | 189/756 [09:06<02:22,  3.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4981696605682373, 'eval_accuracy': 0.2222222222222222, 'eval_f1': 0.008658008658008658, 'eval_runtime': 5.6005, 'eval_samples_per_second': 33.747, 'eval_steps_per_second': 4.285, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 142/142 [01:12<00:00,  1.95it/s]\n",
      "[I 2024-11-30 21:56:35,202] Trial 1 finished with value: 0.23088023088023088 and parameters: {'learning_rate': 1.0143107248236155e-05, 'num_train_epochs': 2, 'seed': 19, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 0.23088023088023088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 72.7923, 'train_samples_per_second': 15.524, 'train_steps_per_second': 1.951, 'train_loss': 3.7333725405410982, 'epoch': 2.0}\n",
      "BestRun(run_id='0', objective=0.23088023088023088, hyperparameters={'learning_rate': 1.0435775984863796e-05, 'num_train_epochs': 1, 'seed': 7, 'per_device_train_batch_size': 4}, run_summary=None)\n"
     ]
    }
   ],
   "source": [
    "# Run hyperparameter search\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "encoded_val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "trainer_hp = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "best_run = trainer_hp.hyperparameter_search(n_trials=2, direction=\"maximize\")\n",
    "\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [09:27<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0689, 'grad_norm': 11.085065841674805, 'learning_rate': 8.88268156424581e-06, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [09:36<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7027, 'grad_norm': 10.091877937316895, 'learning_rate': 7.206703910614526e-06, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [09:46<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4752, 'grad_norm': 9.614151954650879, 'learning_rate': 5.530726256983241e-06, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [09:56<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5799, 'grad_norm': 9.672201156616211, 'learning_rate': 3.854748603351956e-06, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [10:06<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.525, 'grad_norm': 9.818577766418457, 'learning_rate': 2.1787709497206706e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [10:19<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.392, 'grad_norm': 11.35380744934082, 'learning_rate': 5.027932960893855e-07, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      "\n",
      " 25%|██▌       | 189/756 [10:34<02:22,  3.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.304694652557373, 'eval_accuracy': 0.2804232804232804, 'eval_f1': 0.010186430905246973, 'eval_runtime': 8.0175, 'eval_samples_per_second': 23.573, 'eval_steps_per_second': 2.993, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 189/189 [01:28<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 88.6759, 'train_samples_per_second': 8.503, 'train_steps_per_second': 2.131, 'train_loss': 3.6040518523524048, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:06<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results on Test Set: {'eval_loss': 3.1965441703796387, 'eval_accuracy': 0.328042328042328, 'eval_f1': 0.01073965009527109, 'eval_runtime': 6.1758, 'eval_samples_per_second': 30.603, 'eval_steps_per_second': 3.886, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# predictions in test set: 189\n",
      "Predictions on test set:\n",
      "[41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41]\n",
      "True labels:\n",
      "[66 21 43 41 41 59 43 45 54 41 44 61 21 41 74 82 41 41 34 21  7 21 82 61\n",
      " 41 41 41 59 55 41 21 26 82 41 41 77  8 41 48 41 41 78 41 66 44 41 55 80\n",
      " 44 41  3 21 41 21 41 41 69 41 41 20 44 41 41 21 72 21 41 44 45 41 21 44\n",
      " 55 41 72 41  1 44 44 82 66 43 41 31 44 41 22 55 41 68 22 72 41 41 21 82\n",
      " 41 43 77 55 59 50 34 29 41 61 68 21 12 43 10 17 41 41 82 41 21 43 66  3\n",
      " 69 21 41 68 82 41 80 73 41 41 72 46 59 65  6 79 69 21 41 41 59  8 71 70\n",
      " 41 57 11  8 21 41 21 43 44 41 41 54 82 12 41 59 29 41 15 21  2 41 21 41\n",
      " 59 41 59 72 61 24 41 41 41 41 43 54 27 79 41 45 41 21 41 41 59]\n",
      "[{'Class': 41, 'Precision': 0.328042328042328, 'Recall': 1.0}, {'Class': 0, 'Precision': 0, 'Recall': 0}, {'Class': 1, 'Precision': 0, 'Recall': 0.0}, {'Class': 2, 'Precision': 0, 'Recall': 0.0}, {'Class': 3, 'Precision': 0, 'Recall': 0.0}, {'Class': 4, 'Precision': 0, 'Recall': 0}, {'Class': 5, 'Precision': 0, 'Recall': 0}, {'Class': 6, 'Precision': 0, 'Recall': 0.0}, {'Class': 7, 'Precision': 0, 'Recall': 0.0}, {'Class': 8, 'Precision': 0, 'Recall': 0.0}, {'Class': 9, 'Precision': 0, 'Recall': 0}, {'Class': 10, 'Precision': 0, 'Recall': 0.0}, {'Class': 11, 'Precision': 0, 'Recall': 0.0}, {'Class': 12, 'Precision': 0, 'Recall': 0.0}, {'Class': 13, 'Precision': 0, 'Recall': 0}, {'Class': 14, 'Precision': 0, 'Recall': 0}, {'Class': 15, 'Precision': 0, 'Recall': 0.0}, {'Class': 16, 'Precision': 0, 'Recall': 0}, {'Class': 17, 'Precision': 0, 'Recall': 0.0}, {'Class': 18, 'Precision': 0, 'Recall': 0}, {'Class': 19, 'Precision': 0, 'Recall': 0}, {'Class': 20, 'Precision': 0, 'Recall': 0.0}, {'Class': 21, 'Precision': 0, 'Recall': 0.0}, {'Class': 22, 'Precision': 0, 'Recall': 0.0}, {'Class': 23, 'Precision': 0, 'Recall': 0}, {'Class': 24, 'Precision': 0, 'Recall': 0.0}, {'Class': 25, 'Precision': 0, 'Recall': 0}, {'Class': 26, 'Precision': 0, 'Recall': 0.0}, {'Class': 27, 'Precision': 0, 'Recall': 0.0}, {'Class': 28, 'Precision': 0, 'Recall': 0}, {'Class': 29, 'Precision': 0, 'Recall': 0.0}, {'Class': 30, 'Precision': 0, 'Recall': 0}, {'Class': 31, 'Precision': 0, 'Recall': 0.0}, {'Class': 32, 'Precision': 0, 'Recall': 0}, {'Class': 33, 'Precision': 0, 'Recall': 0}, {'Class': 34, 'Precision': 0, 'Recall': 0.0}, {'Class': 35, 'Precision': 0, 'Recall': 0}, {'Class': 36, 'Precision': 0, 'Recall': 0}, {'Class': 37, 'Precision': 0, 'Recall': 0}, {'Class': 38, 'Precision': 0, 'Recall': 0}, {'Class': 39, 'Precision': 0, 'Recall': 0}, {'Class': 40, 'Precision': 0, 'Recall': 0}, {'Class': 42, 'Precision': 0, 'Recall': 0}, {'Class': 43, 'Precision': 0, 'Recall': 0.0}, {'Class': 44, 'Precision': 0, 'Recall': 0.0}, {'Class': 45, 'Precision': 0, 'Recall': 0.0}, {'Class': 46, 'Precision': 0, 'Recall': 0.0}, {'Class': 47, 'Precision': 0, 'Recall': 0}, {'Class': 48, 'Precision': 0, 'Recall': 0.0}, {'Class': 49, 'Precision': 0, 'Recall': 0}, {'Class': 50, 'Precision': 0, 'Recall': 0.0}, {'Class': 51, 'Precision': 0, 'Recall': 0}, {'Class': 52, 'Precision': 0, 'Recall': 0}, {'Class': 53, 'Precision': 0, 'Recall': 0}, {'Class': 54, 'Precision': 0, 'Recall': 0.0}, {'Class': 55, 'Precision': 0, 'Recall': 0.0}, {'Class': 56, 'Precision': 0, 'Recall': 0}, {'Class': 57, 'Precision': 0, 'Recall': 0.0}, {'Class': 58, 'Precision': 0, 'Recall': 0}, {'Class': 59, 'Precision': 0, 'Recall': 0.0}, {'Class': 60, 'Precision': 0, 'Recall': 0}, {'Class': 61, 'Precision': 0, 'Recall': 0.0}, {'Class': 62, 'Precision': 0, 'Recall': 0}, {'Class': 63, 'Precision': 0, 'Recall': 0}, {'Class': 64, 'Precision': 0, 'Recall': 0}, {'Class': 65, 'Precision': 0, 'Recall': 0.0}, {'Class': 66, 'Precision': 0, 'Recall': 0.0}, {'Class': 67, 'Precision': 0, 'Recall': 0}, {'Class': 68, 'Precision': 0, 'Recall': 0.0}, {'Class': 69, 'Precision': 0, 'Recall': 0.0}, {'Class': 70, 'Precision': 0, 'Recall': 0.0}, {'Class': 71, 'Precision': 0, 'Recall': 0.0}, {'Class': 72, 'Precision': 0, 'Recall': 0.0}, {'Class': 73, 'Precision': 0, 'Recall': 0.0}, {'Class': 74, 'Precision': 0, 'Recall': 0.0}, {'Class': 75, 'Precision': 0, 'Recall': 0}, {'Class': 76, 'Precision': 0, 'Recall': 0}, {'Class': 77, 'Precision': 0, 'Recall': 0.0}, {'Class': 78, 'Precision': 0, 'Recall': 0.0}, {'Class': 79, 'Precision': 0, 'Recall': 0.0}, {'Class': 80, 'Precision': 0, 'Recall': 0.0}, {'Class': 81, 'Precision': 0, 'Recall': 0}, {'Class': 82, 'Precision': 0, 'Recall': 0.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model with the best hyperparameters\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer_hp.args, n, v)\n",
    "\n",
    "trainer_hp.train()\n",
    "\n",
    "# Evaluate the model\n",
    "### Evaluate the model\n",
    "results = trainer_hp.evaluate(eval_dataset=encoded_test_dataset)\n",
    "print(\"Evaluation Results on Test Set:\", results)\n",
    "\n",
    "y_pred, y_true = get_predictions_and_labels(encoded_test_dataset, trainer)\n",
    "\n",
    "compute_precision_recall_per_class(y_pred, y_true, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate on oversampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find '/Users/mcelikik/Workspace/movie_script/notebooks/./data/parsed/5thelement.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(balanced_examples)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mfrom_list(balanced_examples)\n\u001b[0;32m---> 22\u001b[0m train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43msplit_dataset_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/parsed/5thelement.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m oversample_dataset(train_dataset)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtended dataset size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_dataset))\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36msplit_dataset_from_json\u001b[0;34m(json_file, test_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mLoad a dataset from a json file and split it into train and test sets by using datasets library from Hugging Face.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    test_size (float): The proportion of the dataset to include in the test split.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load dataset from local json file \u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Split the dataset into a simple train, validation and test sets\u001b[39;00m\n\u001b[1;32m     15\u001b[0m train_test_split \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/load.py:2132\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2128\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2129\u001b[0m )\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2132\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2149\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/load.py:1853\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1852\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1853\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/load.py:1556\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1553\u001b[0m \n\u001b[1;32m   1554\u001b[0m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[0;32m-> 1556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/load.py:942\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[1;32m    937\u001b[0m patterns \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    938\u001b[0m     sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files)\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path, download_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config)\n\u001b[1;32m    941\u001b[0m )\n\u001b[0;32m--> 942\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m supports_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m _MODULE_SUPPORTS_METADATA\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m supports_metadata \u001b[38;5;129;01mand\u001b[39;00m patterns \u001b[38;5;241m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/data_files.py:721\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    716\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    718\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    719\u001b[0m         patterns_for_key\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[0;32m--> 721\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m     )\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/data_files.py:624\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 624\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         )\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/data_files.py:411\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/Users/mcelikik/Workspace/movie_script/notebooks/./data/parsed/5thelement.json'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "def oversample_dataset(dataset):\n",
    "    class_counts = Counter(dataset[\"label\"])\n",
    "\n",
    "    print(class_counts)\n",
    "\n",
    "    max_count = max(class_counts.values())\n",
    "    examples_by_class = {label: [] for label in class_counts}\n",
    "    \n",
    "    for example in dataset:\n",
    "        examples_by_class[example[\"label\"]].append(example)\n",
    "    \n",
    "    balanced_examples = []\n",
    "    for _, examples in examples_by_class.items():\n",
    "        balanced_examples.extend(random.choices(examples, k=max_count))\n",
    "    \n",
    "    random.shuffle(balanced_examples)\n",
    "    return Dataset.from_list(balanced_examples)\n",
    "\n",
    "train_dataset, test_dataset = split_dataset_from_json(\"./data/parsed/5thelement.json\", test_size=0.2)\n",
    "\n",
    "train_dataset = oversample_dataset(train_dataset)\n",
    "\n",
    "print(\"Extended dataset size:\", len(train_dataset))\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "setattr(trainer_hp.args, \"num_train_epochs\", 1)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "### Evaluate the model\n",
    "results = trainer.evaluate(eval_dataset=encoded_test_dataset)\n",
    "print(\"Evaluation Results on Test Set:\", results)\n",
    "\n",
    "y_pred, y_true = get_predictions_and_labels(encoded_test_dataset, trainer)\n",
    "\n",
    "compute_precision_recall_per_class(y_pred, y_true, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
