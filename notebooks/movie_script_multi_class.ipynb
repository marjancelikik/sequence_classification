{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (3.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: xxhash in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (3.11.8)\n",
      "Requirement already satisfied: pandas in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: filelock in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: packaging in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (4.46.3)\n",
      "Requirement already satisfied: requests in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: filelock in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (2.5.1)\n",
      "Requirement already satisfied: fsspec in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: filelock in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: networkx in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pydantic in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (2.10.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pydantic) (2.27.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: no matches found: transformers[torch]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (1.1.1)\n",
      "Requirement already satisfied: pyyaml in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: psutil in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (0.26.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: filelock in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: requests in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (0.26.3)\n",
      "Requirement already satisfied: dill in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: multiprocess in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: packaging in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: xxhash in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: pandas in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (2024.9.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (3.11.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: filelock in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.0.2)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.3/249.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.55.0-cp39-cp39-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pillow>=8 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib) (11.0.0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.55.0 importlib-resources-6.4.5 kiwisolver-1.4.7 matplotlib-3.9.2 pyparsing-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.15.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install pydantic\n",
    "%pip install transformers[torch]\n",
    "%pip install accelerate -U\n",
    "%pip install scikit-learn\n",
    "%pip install evaluate\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the movie script, and save it in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script basic stats:\n",
      "{\n",
      "  \"total_words\": 21849,\n",
      "  \"total_dialogues\": 943,\n",
      "  \"total_words_in_dialogues\": 9457,\n",
      "  \"total_characters\": 83,\n",
      "  \"total_scenes\": 281\n",
      "}\n",
      "Class counts (character speeches):\n",
      "KORBEN with class id 41: 244 (0.25874867444326616)\n",
      "CORNELIUS with class id 21: 99 (0.10498409331919406)\n",
      "ZORG with class id 82: 68 (0.07211028632025451)\n",
      "LOC RHOD with class id 44: 52 (0.05514316012725345)\n",
      "PRESIDENT with class id 59: 48 (0.05090137857900318)\n",
      "LEELOO with class id 43: 48 (0.05090137857900318)\n",
      "MUNRO with class id 55: 36 (0.03817603393425239)\n",
      "PROFESSOR with class id 65: 24 (0.02545068928950159)\n",
      "MACTILBURGH with class id 45: 16 (0.016967126193001062)\n",
      "STAEDERT with class id 72: 15 (0.015906680805938492)\n",
      "CAPTAIN with class id 8: 15 (0.015906680805938492)\n",
      "RIGHT ARM with class id 66: 15 (0.015906680805938492)\n",
      "PRIEST with class id 61: 14 (0.014846235418875928)\n",
      "FINGER (V.O.) with class id 29: 13 (0.013785790031813362)\n",
      "DAVID with class id 22: 12 (0.012725344644750796)\n",
      "VOICE (O.S.) with class id 80: 11 (0.01166489925768823)\n",
      "GIRL with class id 34: 10 (0.010604453870625663)\n",
      "DIVA with class id 24: 10 (0.010604453870625663)\n",
      "SCIENTIST with class id 68: 9 (0.009544008483563097)\n",
      "VOICE with class id 79: 9 (0.009544008483563097)\n",
      "MOTHER (V.O.) with class id 53: 9 (0.009544008483563097)\n",
      "COP with class id 15: 8 (0.008483563096500531)\n",
      "KOMMANDER with class id 40: 7 (0.007423117709437964)\n",
      "AKANIT with class id 1: 7 (0.007423117709437964)\n",
      "CHECK-IN ATTENDANT with class id 10: 7 (0.007423117709437964)\n",
      "MUGGER with class id 54: 6 (0.006362672322375398)\n",
      "THAI with class id 77: 6 (0.006362672322375398)\n",
      "COP 1 with class id 17: 6 (0.006362672322375398)\n",
      "STEWARDESS with class id 73: 6 (0.006362672322375398)\n",
      "PILOT with class id 57: 6 (0.006362672322375398)\n",
      "SHADOW with class id 71: 6 (0.006362672322375398)\n",
      "NEIGHBOR with class id 56: 5 (0.005302226935312832)\n",
      "FOG with class id 31: 5 (0.005302226935312832)\n",
      "TECHNICIAN with class id 76: 4 (0.0042417815482502655)\n",
      "AIDE with class id 0: 4 (0.0042417815482502655)\n",
      "AKNOT with class id 2: 4 (0.0042417815482502655)\n",
      "COPILOT with class id 20: 4 (0.0042417815482502655)\n",
      "SECRETARY (O.S.) with class id 69: 4 (0.0042417815482502655)\n",
      "CHIEF OF POLICE with class id 12: 4 (0.0042417815482502655)\n",
      "BILLY with class id 7: 3 (0.003181336161187699)\n",
      "HEAD SCIENTIST with class id 37: 3 (0.003181336161187699)\n",
      "COMMANDER with class id 14: 3 (0.003181336161187699)\n",
      "MANGALORE with class id 47: 3 (0.003181336161187699)\n",
      "PRESIDENT (O.S.) with class id 60: 2 (0.0021208907741251328)\n",
      "HEAD CHEMISTS with class id 36: 2 (0.0021208907741251328)\n",
      "FIRST OFFICER with class id 30: 2 (0.0021208907741251328)\n",
      "GENERAL MUNRO with class id 33: 2 (0.0021208907741251328)\n",
      "ASSISTANT with class id 3: 2 (0.0021208907741251328)\n",
      "CHIEF with class id 11: 2 (0.0021208907741251328)\n",
      "VOCODER (O.S.) with class id 78: 2 (0.0021208907741251328)\n",
      "STEWARDESS 2 with class id 75: 2 (0.0021208907741251328)\n",
      "MECHANIC with class id 50: 2 (0.0021208907741251328)\n",
      "GROUND CREW MEMBER with class id 35: 2 (0.0021208907741251328)\n",
      "HOSTESS with class id 39: 2 (0.0021208907741251328)\n",
      "MOTHER (O.S.) with class id 51: 2 (0.0021208907741251328)\n",
      "DIVA'S ASSISTANT with class id 25: 2 (0.0021208907741251328)\n",
      "BABY RAY with class id 4: 2 (0.0021208907741251328)\n",
      "MANAGER with class id 46: 2 (0.0021208907741251328)\n",
      "PRIEST (V.O.) with class id 63: 1 (0.0010604453870625664)\n",
      "CLERK with class id 13: 1 (0.0010604453870625664)\n",
      "PRIEST (O.S.) with class id 62: 1 (0.0010604453870625664)\n",
      "GENERAL with class id 32: 1 (0.0010604453870625664)\n",
      "DOCTOR with class id 26: 1 (0.0010604453870625664)\n",
      "VOICE / KORBEN with class id 81: 1 (0.0010604453870625664)\n",
      "COP 3 with class id 19: 1 (0.0010604453870625664)\n",
      "COP (O.S.) with class id 16: 1 (0.0010604453870625664)\n",
      "MOTHER (V.O) with class id 52: 1 (0.0010604453870625664)\n",
      "SECURITY GUARD with class id 70: 1 (0.0010604453870625664)\n",
      "ROBOT with class id 67: 1 (0.0010604453870625664)\n",
      "DAVID (V.O.) with class id 23: 1 (0.0010604453870625664)\n",
      "STEWARDESS 1 with class id 74: 1 (0.0010604453870625664)\n",
      "HOST with class id 38: 1 (0.0010604453870625664)\n",
      "BEAT-UP COP with class id 5: 1 (0.0010604453870625664)\n",
      "CAPTAIN (V.O.) with class id 9: 1 (0.0010604453870625664)\n",
      "MANGALORE 1 with class id 48: 1 (0.0010604453870625664)\n",
      "MANGALORE 2 with class id 49: 1 (0.0010604453870625664)\n",
      "PRINCESS AACHEN with class id 64: 1 (0.0010604453870625664)\n",
      "EMPEROR JAPHET with class id 28: 1 (0.0010604453870625664)\n",
      "EMPEROR with class id 27: 1 (0.0010604453870625664)\n",
      "COP 2 with class id 18: 1 (0.0010604453870625664)\n",
      "POLICEMAN with class id 58: 1 (0.0010604453870625664)\n",
      "BEEPER (O.S.) with class id 6: 1 (0.0010604453870625664)\n",
      "KORBEN (O.S.) with class id 42: 1 (0.0010604453870625664)\n",
      "{\"text\": \"(deciphering) \\\"..when the three planets are in eclipse..\\\" \", \"label\": 65}\n",
      "{\"text\": \"\\\"..the black hole like a door is open... evil comes ... sowing terror and chaos...\\\" see?  the snake, billy.  the ultimate evil ... make sure you get the snake! \", \"label\": 65}\n",
      "{\"text\": \"and when is this door opening snake act supposed to occur? \", \"label\": 7}\n",
      "{\"text\": \"..if this is the five..and this the thousand.. \", \"label\": 65}\n",
      "{\"text\": \"every five thousand years.. \", \"label\": 65}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from  collections import Counter\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data.parse_movie_script import MovieScriptParser\n",
    "\n",
    "# Parse the dataset in structured format and output a json compatible to parse with Hugging face datasets library.\n",
    "# The output will be a json file with the following structure:\n",
    "# { \"text\": ..., \"label\": }\n",
    "parsed_object = MovieScriptParser.from_text_file(\"../data/raw/5thelement.txt\")\n",
    "\n",
    "# Print the stats for the parsed script\n",
    "print(\"Script basic stats:\")\n",
    "print(json.dumps(parsed_object.stats, indent=2))\n",
    "\n",
    "# Print counts per class\n",
    "class_map = parsed_object.get_vocabulary_to_label_mapping()\n",
    "class_counter = Counter()\n",
    "for scene in parsed_object.scenes:\n",
    "    for entry in scene.entries:\n",
    "        if entry.dialogue is not None:\n",
    "            class_counter[entry.dialogue.character] += 1\n",
    "\n",
    "print(\"Class counts (character speeches):\")\n",
    "for key, value in class_counter.most_common():\n",
    "    print(f\"{key} with class id {class_map[key]}: {class_counter[key]} ({class_counter[key] / parsed_object.stats['total_dialogues']})\")\n",
    "\n",
    "# Save the parsed script in json format\n",
    "parsed_object.save_character_dialogue_dataset_in_json_format(\"../data/parsed/5thelement.json\")\n",
    "\n",
    "# Print a few lines of the saved file to see the format\n",
    "with open(\"../data/parsed/5thelement.json\", \"r\") as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a basic traind and test split of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mcelikik/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mcelikik/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 943 examples [00:00, 242652.07 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 754, Test Size: 189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def split_dataset_from_json(json_file: str, test_size: float = 0.2):\n",
    "    \"\"\"\n",
    "    Load a dataset from a json file and split it into train and test sets by using datasets library from Hugging Face.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): The path to the json file containing the dataset.\n",
    "        test_size (float): The proportion of the dataset to include in the test split.\n",
    "    \"\"\"\n",
    "    # Load dataset from local json file \n",
    "    dataset = load_dataset(\"json\", data_files=json_file)\n",
    "\n",
    "    # Split the dataset into a simple train, validation and test sets\n",
    "    train_test_split = dataset['train'].train_test_split(test_size=test_size)\n",
    "    train_dataset = train_test_split['train']\n",
    "    test_dataset = train_test_split['test']\n",
    "\n",
    "    print(f\"Train Size: {len(train_dataset)}, Test Size: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = split_dataset_from_json(\"../data/parsed/5thelement.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a first exploratory Hugging face Transformer model with typical params and do basic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 754/754 [00:00<00:00, 8557.46 examples/s]\n",
      "Map: 100%|██████████| 189/189 [00:00<00:00, 4898.43 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Train Dataset:\n",
      "[[1, 734, 4783, 9069, 328, 1437, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 463, 5, 6675, 9, 41, 20285, 116, 1437, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[21, 21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                               \n",
      " 50%|█████     | 24/48 [00:27<00:19,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.040040493011475, 'eval_accuracy': 0.23809523809523808, 'eval_f1': 0.008547008547008548, 'eval_runtime': 4.9674, 'eval_samples_per_second': 38.048, 'eval_steps_per_second': 4.832, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 30/48 [00:41<00:31,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2404, 'grad_norm': 7.257016658782959, 'learning_rate': 4.736842105263158e-06, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 48/48 [01:26<00:00,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.765052080154419, 'eval_accuracy': 0.23809523809523808, 'eval_f1': 0.008547008547008548, 'eval_runtime': 5.9513, 'eval_samples_per_second': 31.758, 'eval_steps_per_second': 4.033, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [01:31<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 91.8754, 'train_samples_per_second': 16.414, 'train_steps_per_second': 0.522, 'train_loss': 4.055851300557454, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=48, training_loss=4.055851300557454, metrics={'train_runtime': 91.8754, 'train_samples_per_second': 16.414, 'train_steps_per_second': 0.522, 'total_flos': 115659434542080.0, 'train_loss': 4.055851300557454, 'epoch': 2.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "### Train model\n",
    "model_name = \"microsoft/deberta-base\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add a padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use <EOS> as <PAD>\n",
    "\n",
    "# Tokenize the data\n",
    "def preprocess_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],  # raw text to be tokenized\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = examples[\"label\"]  # add the labels to the tokenized input\n",
    "    return tokenized\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Sanity check: print a few examples of the encoded dataset\n",
    "print(\"Encoded Train Dataset:\")\n",
    "print(encoded_train_dataset[\"input_ids\"][:2])\n",
    "print(encoded_train_dataset[\"attention_mask\"][:2])\n",
    "print(encoded_train_dataset[\"labels\"][:2])\n",
    "\n",
    "# Load the model\n",
    "num_labels = len(parsed_object.character_vocabulary)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Add a padding token if not present\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=30,\n",
    "    warmup_steps=10,  # learning rate will be gradually increased during the first 10 steps\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    metric_acc = evaluate.load(\"accuracy\")\n",
    "    metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "    accuracy = metric_acc.compute(predictions=predictions, references=labels)\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute confusion matrix and additional metrics per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:04<00:00,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# predictions in test set: 189\n",
      "Predictions on test set:\n",
      "[41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41]\n",
      "True labels:\n",
      "[41 66 69 31  8 55 82 80 61 82 22 69 82 82 41 55 37 43 43 59 59 41 59 59\n",
      " 41 41 59 21 41 41 23 45  8 41 41 41 44 55 53 44 59 21 66 53 61 41 21 41\n",
      " 59 22 55 82 24 41 43 41 59 21 10 15 55 57 43  8  1 41 41 65 82 82 41 41\n",
      " 41 43 43 21 59 41 82 41 68 65 41 13 57 73 82 41 21 43 41 21 24 41 53  1\n",
      " 45 47 21 29 21 41 41 35 21 77 41 55 41 60 41  1 24 82 61 79 54 80 44 73\n",
      " 44 75 12 45 79  8 65 41 82 72 41 57 48 61  2 21 43 41 21 35 41 43 17 43\n",
      " 43 41 59 27  8 61 77 54 41 41 78 55 21 41 72 44 56 41 43 41  1 41  8 14\n",
      " 44 21 41 21 55 41 72 44 41 68  0 66 55 29  0 65 43 21 41 55 59]\n",
      "Precision and recall per class:\n",
      "[{'Class': 41, 'Precision': 0.23809523809523808, 'Recall': 1.0}, {'Class': 0, 'Precision': 0, 'Recall': 0.0}, {'Class': 1, 'Precision': 0, 'Recall': 0.0}, {'Class': 2, 'Precision': 0, 'Recall': 0.0}, {'Class': 3, 'Precision': 0, 'Recall': 0}, {'Class': 4, 'Precision': 0, 'Recall': 0}, {'Class': 5, 'Precision': 0, 'Recall': 0}, {'Class': 6, 'Precision': 0, 'Recall': 0}, {'Class': 7, 'Precision': 0, 'Recall': 0}, {'Class': 8, 'Precision': 0, 'Recall': 0.0}, {'Class': 9, 'Precision': 0, 'Recall': 0}, {'Class': 10, 'Precision': 0, 'Recall': 0.0}, {'Class': 11, 'Precision': 0, 'Recall': 0}, {'Class': 12, 'Precision': 0, 'Recall': 0.0}, {'Class': 13, 'Precision': 0, 'Recall': 0.0}, {'Class': 14, 'Precision': 0, 'Recall': 0.0}, {'Class': 15, 'Precision': 0, 'Recall': 0.0}, {'Class': 16, 'Precision': 0, 'Recall': 0}, {'Class': 17, 'Precision': 0, 'Recall': 0.0}, {'Class': 18, 'Precision': 0, 'Recall': 0}, {'Class': 19, 'Precision': 0, 'Recall': 0}, {'Class': 20, 'Precision': 0, 'Recall': 0}, {'Class': 21, 'Precision': 0, 'Recall': 0.0}, {'Class': 22, 'Precision': 0, 'Recall': 0.0}, {'Class': 23, 'Precision': 0, 'Recall': 0.0}, {'Class': 24, 'Precision': 0, 'Recall': 0.0}, {'Class': 25, 'Precision': 0, 'Recall': 0}, {'Class': 26, 'Precision': 0, 'Recall': 0}, {'Class': 27, 'Precision': 0, 'Recall': 0.0}, {'Class': 28, 'Precision': 0, 'Recall': 0}, {'Class': 29, 'Precision': 0, 'Recall': 0.0}, {'Class': 30, 'Precision': 0, 'Recall': 0}, {'Class': 31, 'Precision': 0, 'Recall': 0.0}, {'Class': 32, 'Precision': 0, 'Recall': 0}, {'Class': 33, 'Precision': 0, 'Recall': 0}, {'Class': 34, 'Precision': 0, 'Recall': 0}, {'Class': 35, 'Precision': 0, 'Recall': 0.0}, {'Class': 36, 'Precision': 0, 'Recall': 0}, {'Class': 37, 'Precision': 0, 'Recall': 0.0}, {'Class': 38, 'Precision': 0, 'Recall': 0}, {'Class': 39, 'Precision': 0, 'Recall': 0}, {'Class': 40, 'Precision': 0, 'Recall': 0}, {'Class': 42, 'Precision': 0, 'Recall': 0}, {'Class': 43, 'Precision': 0, 'Recall': 0.0}, {'Class': 44, 'Precision': 0, 'Recall': 0.0}, {'Class': 45, 'Precision': 0, 'Recall': 0.0}, {'Class': 46, 'Precision': 0, 'Recall': 0}, {'Class': 47, 'Precision': 0, 'Recall': 0.0}, {'Class': 48, 'Precision': 0, 'Recall': 0.0}, {'Class': 49, 'Precision': 0, 'Recall': 0}, {'Class': 50, 'Precision': 0, 'Recall': 0}, {'Class': 51, 'Precision': 0, 'Recall': 0}, {'Class': 52, 'Precision': 0, 'Recall': 0}, {'Class': 53, 'Precision': 0, 'Recall': 0.0}, {'Class': 54, 'Precision': 0, 'Recall': 0.0}, {'Class': 55, 'Precision': 0, 'Recall': 0.0}, {'Class': 56, 'Precision': 0, 'Recall': 0.0}, {'Class': 57, 'Precision': 0, 'Recall': 0.0}, {'Class': 58, 'Precision': 0, 'Recall': 0}, {'Class': 59, 'Precision': 0, 'Recall': 0.0}, {'Class': 60, 'Precision': 0, 'Recall': 0.0}, {'Class': 61, 'Precision': 0, 'Recall': 0.0}, {'Class': 62, 'Precision': 0, 'Recall': 0}, {'Class': 63, 'Precision': 0, 'Recall': 0}, {'Class': 64, 'Precision': 0, 'Recall': 0}, {'Class': 65, 'Precision': 0, 'Recall': 0.0}, {'Class': 66, 'Precision': 0, 'Recall': 0.0}, {'Class': 67, 'Precision': 0, 'Recall': 0}, {'Class': 68, 'Precision': 0, 'Recall': 0.0}, {'Class': 69, 'Precision': 0, 'Recall': 0.0}, {'Class': 70, 'Precision': 0, 'Recall': 0}, {'Class': 71, 'Precision': 0, 'Recall': 0}, {'Class': 72, 'Precision': 0, 'Recall': 0.0}, {'Class': 73, 'Precision': 0, 'Recall': 0.0}, {'Class': 74, 'Precision': 0, 'Recall': 0}, {'Class': 75, 'Precision': 0, 'Recall': 0.0}, {'Class': 76, 'Precision': 0, 'Recall': 0}, {'Class': 77, 'Precision': 0, 'Recall': 0.0}, {'Class': 78, 'Precision': 0, 'Recall': 0.0}, {'Class': 79, 'Precision': 0, 'Recall': 0.0}, {'Class': 80, 'Precision': 0, 'Recall': 0.0}, {'Class': 81, 'Precision': 0, 'Recall': 0}, {'Class': 82, 'Precision': 0, 'Recall': 0.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions on test set to compute additional metrics\n",
    "def get_predictions_and_labels(test_dataset, trainer):\n",
    "    import numpy as np\n",
    "\n",
    "    # Predict on the evaluation dataset\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "\n",
    "    # Extract predictions and true labels\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    y_true = predictions.label_ids\n",
    "    return y_pred, y_true\n",
    "\n",
    "\n",
    "def compute_precision_recall_per_class(y_pred, y_true, num_labels):\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "\n",
    "    def compute_precision_recall(confusion_matrix):\n",
    "        \"\"\"\n",
    "        Compute precision and recall for each class from a confusion matrix.\n",
    "        \n",
    "        :param confusion_matrix: 2D array, where rows are actual classes\n",
    "                                and columns are predicted classes.\n",
    "        :return: Dictionary with precision and recall for each class.\n",
    "        \"\"\"\n",
    "        num_classes = confusion_matrix.shape[0]\n",
    "        metrics = []\n",
    "        \n",
    "        for i in range(num_classes):\n",
    "            # True Positives (diagonal element)\n",
    "            TP = confusion_matrix[i, i]\n",
    "            \n",
    "            # False Positives (sum of column i, excluding TP)\n",
    "            FP = np.sum(confusion_matrix[:, i]) - TP\n",
    "            \n",
    "            # False Negatives (sum of row i, excluding TP)\n",
    "            FN = np.sum(confusion_matrix[i, :]) - TP\n",
    "            \n",
    "            # Precision and Recall\n",
    "            precision = float(TP / (TP + FP)) if (TP + FP) > 0 else 0\n",
    "            recall = float(TP / (TP + FN)) if (TP + FN) > 0 else 0\n",
    "            \n",
    "            metrics.append({'Class': i, 'Precision': precision, 'Recall': recall})\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    print(\"# predictions in test set: {}\".format(len(y_pred)))\n",
    "    print(\"Predictions on test set:\")\n",
    "    print(y_pred)\n",
    "    print(\"True labels:\")\n",
    "    print(y_true)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(num_labels))\n",
    "\n",
    "    # Compute precision and recall per class\n",
    "    metrics_per_class = compute_precision_recall(cm)\n",
    "    metrics_per_class = sorted(metrics_per_class, key=lambda x: x['Recall'], reverse=True)\n",
    "    \n",
    "    print(\"Precision and recall per class:\")\n",
    "    print(metrics_per_class)\n",
    "\n",
    "y_pred, y_true = get_predictions_and_labels(encoded_test_dataset, trainer)\n",
    "compute_precision_recall_per_class(y_pred, y_true, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate on oversampled dataset (oversample each class proportionally to max class size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 754\n",
      "Oversampled training dataset size: 7618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7618/7618 [00:00<00:00, 13454.65 examples/s]\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  6%|▋         | 30/478 [00:27<07:28,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3595, 'grad_norm': 3.3645477294921875, 'learning_rate': 9.572649572649575e-06, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 60/478 [00:56<06:18,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.122, 'grad_norm': 3.8560879230499268, 'learning_rate': 8.931623931623933e-06, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 90/478 [01:22<05:27,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8841, 'grad_norm': 4.209497928619385, 'learning_rate': 8.290598290598293e-06, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 120/478 [01:53<05:37,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6739, 'grad_norm': 4.871729373931885, 'learning_rate': 7.649572649572649e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 150/478 [02:19<04:37,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4838, 'grad_norm': 5.105074405670166, 'learning_rate': 7.008547008547009e-06, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 180/478 [02:48<04:28,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2759, 'grad_norm': 5.372707366943359, 'learning_rate': 6.367521367521368e-06, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 210/478 [03:14<03:49,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0938, 'grad_norm': 5.992202281951904, 'learning_rate': 5.726495726495727e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 50%|█████     | 239/478 [03:50<03:38,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.8622593879699707, 'eval_accuracy': 0.06349206349206349, 'eval_f1': 0.05572486772486772, 'eval_runtime': 5.3122, 'eval_samples_per_second': 35.578, 'eval_steps_per_second': 4.518, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 240/478 [04:06<28:52,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8349, 'grad_norm': 5.542602062225342, 'learning_rate': 5.085470085470086e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 270/478 [04:36<04:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6352, 'grad_norm': 5.297227382659912, 'learning_rate': 4.444444444444444e-06, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 300/478 [05:04<02:55,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5431, 'grad_norm': 5.606707572937012, 'learning_rate': 3.8034188034188036e-06, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 330/478 [05:33<02:51,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.395, 'grad_norm': 5.65814208984375, 'learning_rate': 3.1623931623931626e-06, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 360/478 [05:59<01:42,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3085, 'grad_norm': 6.448210716247559, 'learning_rate': 2.5213675213675216e-06, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 390/478 [06:26<01:32,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2555, 'grad_norm': 6.278285980224609, 'learning_rate': 1.8803418803418804e-06, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 420/478 [06:56<00:55,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1433, 'grad_norm': 6.010166645050049, 'learning_rate': 1.2393162393162394e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 450/478 [07:24<00:26,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.15, 'grad_norm': 6.3843674659729, 'learning_rate': 5.982905982905984e-07, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 478/478 [08:11<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7538673877716064, 'eval_accuracy': 0.07936507936507936, 'eval_f1': 0.06009914529914529, 'eval_runtime': 8.2946, 'eval_samples_per_second': 22.786, 'eval_steps_per_second': 2.893, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 478/478 [08:16<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 496.0503, 'train_samples_per_second': 30.715, 'train_steps_per_second': 0.964, 'train_loss': 2.958762731512221, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:04<00:00,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# predictions in test set: 189\n",
      "Predictions on test set:\n",
      "[54 10 17 76 44 53 10 80 24 54 24 57 20 40 56 12 10 24 24 54  1 24 10 10\n",
      " 53 53  1 45 24 54 54 17 17 15 54 10 53 53 53 53 44 34 10 53 24 24 40 10\n",
      " 10 10 10 54 24 34 34 24 45 40 79 76 24 34 53 76 12 31 17 65 17 45 34 10\n",
      " 56 57 34 10 45 12 72 53 68 65 53 15 57 24  2 24 40 31 53 24 24 10 53 40\n",
      " 68 76 34 79 31 53 31 31 53 65 45 10 43  1 56 53 24 53 65 20 53 40 29 31\n",
      " 10 10 10 45 79 80 65 45 34 40 71 45 54 56 45 40 54 53 10 57 10 54 76 24\n",
      " 34 31 10 76 76 34 29 71 24 53 57 53 71 10 10  1 10 53  1 71  2 10 45 76\n",
      " 10 24 29 24 40 10 57  1 71 68 68 34 22 53 12 24 10 65 53 24 76]\n",
      "True labels:\n",
      "[41 66 69 31  8 55 82 80 61 82 22 69 82 82 41 55 37 43 43 59 59 41 59 59\n",
      " 41 41 59 21 41 41 23 45  8 41 41 41 44 55 53 44 59 21 66 53 61 41 21 41\n",
      " 59 22 55 82 24 41 43 41 59 21 10 15 55 57 43  8  1 41 41 65 82 82 41 41\n",
      " 41 43 43 21 59 41 82 41 68 65 41 13 57 73 82 41 21 43 41 21 24 41 53  1\n",
      " 45 47 21 29 21 41 41 35 21 77 41 55 41 60 41  1 24 82 61 79 54 80 44 73\n",
      " 44 75 12 45 79  8 65 41 82 72 41 57 48 61  2 21 43 41 21 35 41 43 17 43\n",
      " 43 41 59 27  8 61 77 54 41 41 78 55 21 41 72 44 56 41 43 41  1 41  8 14\n",
      " 44 21 41 21 55 41 72 44 41 68  0 66 55 29  0 65 43 21 41 55 59]\n",
      "Precision and recall per class:\n",
      "[{'Class': 24, 'Precision': 0.13636363636363635, 'Recall': 1.0}, {'Class': 53, 'Precision': 0.125, 'Recall': 1.0}, {'Class': 68, 'Precision': 0.5, 'Recall': 1.0}, {'Class': 65, 'Precision': 0.5, 'Recall': 0.75}, {'Class': 79, 'Precision': 0.3333333333333333, 'Recall': 0.5}, {'Class': 80, 'Precision': 0.5, 'Recall': 0.5}, {'Class': 45, 'Precision': 0.1, 'Recall': 0.3333333333333333}, {'Class': 57, 'Precision': 0.16666666666666666, 'Recall': 0.3333333333333333}, {'Class': 0, 'Precision': 0, 'Recall': 0.0}, {'Class': 1, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 2, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 3, 'Precision': 0, 'Recall': 0}, {'Class': 4, 'Precision': 0, 'Recall': 0}, {'Class': 5, 'Precision': 0, 'Recall': 0}, {'Class': 6, 'Precision': 0, 'Recall': 0}, {'Class': 7, 'Precision': 0, 'Recall': 0}, {'Class': 8, 'Precision': 0, 'Recall': 0.0}, {'Class': 9, 'Precision': 0, 'Recall': 0}, {'Class': 10, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 11, 'Precision': 0, 'Recall': 0}, {'Class': 12, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 13, 'Precision': 0, 'Recall': 0.0}, {'Class': 14, 'Precision': 0, 'Recall': 0.0}, {'Class': 15, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 16, 'Precision': 0, 'Recall': 0}, {'Class': 17, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 18, 'Precision': 0, 'Recall': 0}, {'Class': 19, 'Precision': 0, 'Recall': 0}, {'Class': 20, 'Precision': 0.0, 'Recall': 0}, {'Class': 21, 'Precision': 0, 'Recall': 0.0}, {'Class': 22, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 23, 'Precision': 0, 'Recall': 0.0}, {'Class': 25, 'Precision': 0, 'Recall': 0}, {'Class': 26, 'Precision': 0, 'Recall': 0}, {'Class': 27, 'Precision': 0, 'Recall': 0.0}, {'Class': 28, 'Precision': 0, 'Recall': 0}, {'Class': 29, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 30, 'Precision': 0, 'Recall': 0}, {'Class': 31, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 32, 'Precision': 0, 'Recall': 0}, {'Class': 33, 'Precision': 0, 'Recall': 0}, {'Class': 34, 'Precision': 0.0, 'Recall': 0}, {'Class': 35, 'Precision': 0, 'Recall': 0.0}, {'Class': 36, 'Precision': 0, 'Recall': 0}, {'Class': 37, 'Precision': 0, 'Recall': 0.0}, {'Class': 38, 'Precision': 0, 'Recall': 0}, {'Class': 39, 'Precision': 0, 'Recall': 0}, {'Class': 40, 'Precision': 0.0, 'Recall': 0}, {'Class': 41, 'Precision': 0, 'Recall': 0.0}, {'Class': 42, 'Precision': 0, 'Recall': 0}, {'Class': 43, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 44, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 46, 'Precision': 0, 'Recall': 0}, {'Class': 47, 'Precision': 0, 'Recall': 0.0}, {'Class': 48, 'Precision': 0, 'Recall': 0.0}, {'Class': 49, 'Precision': 0, 'Recall': 0}, {'Class': 50, 'Precision': 0, 'Recall': 0}, {'Class': 51, 'Precision': 0, 'Recall': 0}, {'Class': 52, 'Precision': 0, 'Recall': 0}, {'Class': 54, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 55, 'Precision': 0, 'Recall': 0.0}, {'Class': 56, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 58, 'Precision': 0, 'Recall': 0}, {'Class': 59, 'Precision': 0, 'Recall': 0.0}, {'Class': 60, 'Precision': 0, 'Recall': 0.0}, {'Class': 61, 'Precision': 0, 'Recall': 0.0}, {'Class': 62, 'Precision': 0, 'Recall': 0}, {'Class': 63, 'Precision': 0, 'Recall': 0}, {'Class': 64, 'Precision': 0, 'Recall': 0}, {'Class': 66, 'Precision': 0, 'Recall': 0.0}, {'Class': 67, 'Precision': 0, 'Recall': 0}, {'Class': 69, 'Precision': 0, 'Recall': 0.0}, {'Class': 70, 'Precision': 0, 'Recall': 0}, {'Class': 71, 'Precision': 0.0, 'Recall': 0}, {'Class': 72, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 73, 'Precision': 0, 'Recall': 0.0}, {'Class': 74, 'Precision': 0, 'Recall': 0}, {'Class': 75, 'Precision': 0, 'Recall': 0.0}, {'Class': 76, 'Precision': 0.0, 'Recall': 0}, {'Class': 77, 'Precision': 0, 'Recall': 0.0}, {'Class': 78, 'Precision': 0, 'Recall': 0.0}, {'Class': 81, 'Precision': 0, 'Recall': 0}, {'Class': 82, 'Precision': 0, 'Recall': 0.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "def oversample_dataset(dataset: Dataset, class_count_threshold=0):\n",
    "    \"\"\"\n",
    "    Oversamples the dataset to balance the class distribution. Ignore classes with count less or equal to class_count_threshold.\n",
    "    Args:\n",
    "        dataset (Dataset): A dataset object containing examples with a \"label\" field.\n",
    "        class_count_threshold (int): The minimum count of a class to be considered for oversampling.\n",
    "    Returns:\n",
    "        Dataset: A new dataset object with balanced class distribution by oversampling the minority classes.\n",
    "    \"\"\"\n",
    "    class_counts = Counter(dataset[\"label\"])\n",
    "\n",
    "    max_count = max(class_counts.values())\n",
    "    examples_by_class = {label: [] for label in class_counts}\n",
    "    \n",
    "    for example in dataset:\n",
    "        examples_by_class[example[\"label\"]].append(example)\n",
    "    \n",
    "    balanced_examples = []\n",
    "    for _, examples in examples_by_class.items():\n",
    "        if len(examples) > class_count_threshold:\n",
    "            balanced_examples.extend(random.choices(examples, k=max_count))\n",
    "        else:\n",
    "            balanced_examples.extend(examples)\n",
    "    \n",
    "    random.shuffle(balanced_examples)\n",
    "    return Dataset.from_list(balanced_examples)\n",
    "\n",
    "train_dataset_oversampled = oversample_dataset(train_dataset, class_count_threshold=2)\n",
    "\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Oversampled training dataset size:\", len(train_dataset_oversampled))\n",
    "\n",
    "encoded_train_dataset_oversampled = train_dataset_oversampled.map(preprocess_function, batched=True)\n",
    "\n",
    "# Reset model weights to previous state\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset_oversampled,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred, y_true = get_predictions_and_labels(encoded_test_dataset, trainer)\n",
    "compute_precision_recall_per_class(y_pred, y_true, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use weighted cross-entropy loss to simulate oversampling with less computational effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 21 weight: 2.513157894736842\n",
      "Class 43 weight: 4.5476190476190474\n",
      "Class 44 weight: 4.2444444444444445\n",
      "Class 45 weight: 14.692307692307692\n",
      "Class 55 weight: 6.586206896551724\n",
      "Class 59 weight: 5.96875\n",
      "Class 65 weight: 9.095238095238095\n",
      "Class 66 weight: 15.916666666666666\n",
      "Class 82 weight: 3.3508771929824563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                      \n",
      "\u001b[A                                             \n",
      "\n",
      "  0%|          | 0/48 [15:39<?, ?it/s]         \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.153419017791748, 'eval_accuracy': 0.06878306878306878, 'eval_f1': 0.0058285248003730255, 'eval_runtime': 4.1092, 'eval_samples_per_second': 45.995, 'eval_steps_per_second': 5.841, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \n",
      "  0%|          | 0/48 [15:52<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3112, 'grad_norm': 5.881208896636963, 'learning_rate': 9.130434782608697e-06, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                      \n",
      "\u001b[A                                             \n",
      "\n",
      "  0%|          | 0/48 [16:11<?, ?it/s]         \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.6875429153442383, 'eval_accuracy': 0.05291005291005291, 'eval_f1': 0.0023929169657812875, 'eval_runtime': 3.6739, 'eval_samples_per_second': 51.443, 'eval_steps_per_second': 6.532, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \n",
      "  0%|          | 0/48 [16:39<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7329, 'grad_norm': 7.0592360496521, 'learning_rate': 7.82608695652174e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                      \n",
      "\u001b[A                                             \n",
      "\n",
      "  0%|          | 0/48 [16:53<?, ?it/s]         \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.431457281112671, 'eval_accuracy': 0.07936507936507936, 'eval_f1': 0.006308713961775186, 'eval_runtime': 3.8201, 'eval_samples_per_second': 49.475, 'eval_steps_per_second': 6.282, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \n",
      "  0%|          | 0/48 [17:17<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4405, 'grad_norm': 6.139218330383301, 'learning_rate': 6.521739130434783e-06, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                      \n",
      "\u001b[A                                             \n",
      "\n",
      "  0%|          | 0/48 [17:28<?, ?it/s]         \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2895896434783936, 'eval_accuracy': 0.05291005291005291, 'eval_f1': 0.004029872818692695, 'eval_runtime': 4.7263, 'eval_samples_per_second': 39.989, 'eval_steps_per_second': 5.078, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \n",
      "  0%|          | 0/48 [18:08<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2011, 'grad_norm': 8.90644359588623, 'learning_rate': 5.2173913043478265e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                      \n",
      "\u001b[A                                              \n",
      "\n",
      "  0%|          | 0/48 [18:12<?, ?it/s]         \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.19889235496521, 'eval_accuracy': 0.06878306878306878, 'eval_f1': 0.008816990114722121, 'eval_runtime': 3.9096, 'eval_samples_per_second': 48.342, 'eval_steps_per_second': 6.139, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                      \n",
      "\u001b[A                                              \n",
      "\n",
      "  0%|          | 0/48 [19:07<?, ?it/s]         \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1331732273101807, 'eval_accuracy': 0.07936507936507936, 'eval_f1': 0.015170257281520392, 'eval_runtime': 3.9535, 'eval_samples_per_second': 47.805, 'eval_steps_per_second': 6.071, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \n",
      "  0%|          | 0/48 [19:29<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0861, 'grad_norm': 6.414454936981201, 'learning_rate': 3.91304347826087e-06, 'epoch': 6.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                      \n",
      "\u001b[A                                              \n",
      "\n",
      "  0%|          | 0/48 [19:49<?, ?it/s]         \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.094181537628174, 'eval_accuracy': 0.10582010582010581, 'eval_f1': 0.018575026560055923, 'eval_runtime': 3.8474, 'eval_samples_per_second': 49.125, 'eval_steps_per_second': 6.238, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \n",
      "  0%|          | 0/48 [20:08<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9963, 'grad_norm': 6.014962196350098, 'learning_rate': 2.6086956521739132e-06, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                      \n",
      "\u001b[A                                              \n",
      "\n",
      "  0%|          | 0/48 [20:27<?, ?it/s]         \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.063671827316284, 'eval_accuracy': 0.08994708994708994, 'eval_f1': 0.021797323626161984, 'eval_runtime': 6.5609, 'eval_samples_per_second': 28.807, 'eval_steps_per_second': 3.658, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \n",
      "  0%|          | 0/48 [21:13<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9201, 'grad_norm': 8.563982963562012, 'learning_rate': 1.3043478260869566e-06, 'epoch': 8.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                      \n",
      "\u001b[A                                              \n",
      "\n",
      "  0%|          | 0/48 [21:23<?, ?it/s]         \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.037750720977783, 'eval_accuracy': 0.12169312169312169, 'eval_f1': 0.0336792898977773, 'eval_runtime': 4.6351, 'eval_samples_per_second': 40.775, 'eval_steps_per_second': 5.178, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \n",
      "  0%|          | 0/48 [21:50<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8736, 'grad_norm': 7.86387825012207, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                      \n",
      "\u001b[A                                              \n",
      "\n",
      "  0%|          | 0/48 [22:00<?, ?it/s]         \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0337133407592773, 'eval_accuracy': 0.1111111111111111, 'eval_f1': 0.0308969131510387, 'eval_runtime': 5.3574, 'eval_samples_per_second': 35.278, 'eval_steps_per_second': 4.48, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \n",
      "100%|██████████| 240/240 [07:11<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 431.9662, 'train_samples_per_second': 17.455, 'train_steps_per_second': 0.556, 'train_loss': 3.3202081044514973, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:04<00:00,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# predictions in test set: 189\n",
      "Predictions on test set:\n",
      "[44 43 59 59 41 65 44 59 41 44 44 59 44 65 44 59 44 41 44 45 44 44 44 59\n",
      " 45 59 59 21 59 59 59 59 82 59 59 45 44 21 59 59 59 21 21 59 44 45 45 59\n",
      " 59 45 44 44 59 44 59 44 59 65 59 44 45 44 59 44 59 21 21 59 43 59 59 21\n",
      " 59 59 59 44 59 59 21 44 43 44 45 59 44 44 59 44 43 45 59 44 59 21 44 82\n",
      " 21 82 59 21 59 41 59 44 59 82 41 59 45 82 82 44 82 44 44 59 21 59 44 44\n",
      " 59 82 82 21 44 82 59 59 45 43 43 82 59 59 41 44 44 59 59 59 44 59 21 65\n",
      " 59 44 44 44 44 21 59 59 21 59 59 59 59 44 43 59 21 41 59 59 82 59 59 59\n",
      " 44 41 59 21 82 21 59 21 59 44 44 59 59 44 41 59 59 59 45 59 59]\n",
      "True labels:\n",
      "[41 34 22 45  0 21 41 80 56 66 82 41 41 65 76 41 41 82 82 59 44 55 41 55\n",
      " 21  1 41 73 66 71 59 61 82 21 72 68 41 59 55 64 21 66 21 43 41 59 24 76\n",
      " 41 55 41 41 41 41  7 77 41 21 59 41 21 41 41 44  4 43 43 82 59 43 21 82\n",
      " 80 55 68 82 41 79 21 41 43 21 82 59 41 73 44 41 41 40 41 29 61 21 41 80\n",
      " 33 39 41 56 55 21 72 29 43 66 65 41 68 41 21 24 41 53 82 59 72 21 21 45\n",
      "  8 24 21 44 29 68 43 82 17 41 43 44 65 17 34 77 41 70 43 65 44 41 41 65\n",
      " 21 53 21 41 55 33 41 12 40 43 41 10 21 41 43 59 55 41 43  8 41 15 41 41\n",
      " 55 22 20 61 43 43 35 21 41 44 44 21 45 41 59 41 41 44 55 58 65]\n",
      "Precision and recall per class:\n",
      "[{'Class': 44, 'Precision': 0.10638297872340426, 'Recall': 0.5555555555555556}, {'Class': 59, 'Precision': 0.06493506493506493, 'Recall': 0.5}, {'Class': 65, 'Precision': 0.5, 'Recall': 0.3333333333333333}, {'Class': 43, 'Precision': 0.42857142857142855, 'Recall': 0.21428571428571427}, {'Class': 21, 'Precision': 0.2, 'Recall': 0.19047619047619047}, {'Class': 82, 'Precision': 0.07692307692307693, 'Recall': 0.1}, {'Class': 41, 'Precision': 0.1111111111111111, 'Recall': 0.021739130434782608}, {'Class': 0, 'Precision': 0, 'Recall': 0.0}, {'Class': 1, 'Precision': 0, 'Recall': 0.0}, {'Class': 2, 'Precision': 0, 'Recall': 0}, {'Class': 3, 'Precision': 0, 'Recall': 0}, {'Class': 4, 'Precision': 0, 'Recall': 0.0}, {'Class': 5, 'Precision': 0, 'Recall': 0}, {'Class': 6, 'Precision': 0, 'Recall': 0}, {'Class': 7, 'Precision': 0, 'Recall': 0.0}, {'Class': 8, 'Precision': 0, 'Recall': 0.0}, {'Class': 9, 'Precision': 0, 'Recall': 0}, {'Class': 10, 'Precision': 0, 'Recall': 0.0}, {'Class': 11, 'Precision': 0, 'Recall': 0}, {'Class': 12, 'Precision': 0, 'Recall': 0.0}, {'Class': 13, 'Precision': 0, 'Recall': 0}, {'Class': 14, 'Precision': 0, 'Recall': 0}, {'Class': 15, 'Precision': 0, 'Recall': 0.0}, {'Class': 16, 'Precision': 0, 'Recall': 0}, {'Class': 17, 'Precision': 0, 'Recall': 0.0}, {'Class': 18, 'Precision': 0, 'Recall': 0}, {'Class': 19, 'Precision': 0, 'Recall': 0}, {'Class': 20, 'Precision': 0, 'Recall': 0.0}, {'Class': 22, 'Precision': 0, 'Recall': 0.0}, {'Class': 23, 'Precision': 0, 'Recall': 0}, {'Class': 24, 'Precision': 0, 'Recall': 0.0}, {'Class': 25, 'Precision': 0, 'Recall': 0}, {'Class': 26, 'Precision': 0, 'Recall': 0}, {'Class': 27, 'Precision': 0, 'Recall': 0}, {'Class': 28, 'Precision': 0, 'Recall': 0}, {'Class': 29, 'Precision': 0, 'Recall': 0.0}, {'Class': 30, 'Precision': 0, 'Recall': 0}, {'Class': 31, 'Precision': 0, 'Recall': 0}, {'Class': 32, 'Precision': 0, 'Recall': 0}, {'Class': 33, 'Precision': 0, 'Recall': 0.0}, {'Class': 34, 'Precision': 0, 'Recall': 0.0}, {'Class': 35, 'Precision': 0, 'Recall': 0.0}, {'Class': 36, 'Precision': 0, 'Recall': 0}, {'Class': 37, 'Precision': 0, 'Recall': 0}, {'Class': 38, 'Precision': 0, 'Recall': 0}, {'Class': 39, 'Precision': 0, 'Recall': 0.0}, {'Class': 40, 'Precision': 0, 'Recall': 0.0}, {'Class': 42, 'Precision': 0, 'Recall': 0}, {'Class': 45, 'Precision': 0.0, 'Recall': 0.0}, {'Class': 46, 'Precision': 0, 'Recall': 0}, {'Class': 47, 'Precision': 0, 'Recall': 0}, {'Class': 48, 'Precision': 0, 'Recall': 0}, {'Class': 49, 'Precision': 0, 'Recall': 0}, {'Class': 50, 'Precision': 0, 'Recall': 0}, {'Class': 51, 'Precision': 0, 'Recall': 0}, {'Class': 52, 'Precision': 0, 'Recall': 0}, {'Class': 53, 'Precision': 0, 'Recall': 0.0}, {'Class': 54, 'Precision': 0, 'Recall': 0}, {'Class': 55, 'Precision': 0, 'Recall': 0.0}, {'Class': 56, 'Precision': 0, 'Recall': 0.0}, {'Class': 57, 'Precision': 0, 'Recall': 0}, {'Class': 58, 'Precision': 0, 'Recall': 0.0}, {'Class': 60, 'Precision': 0, 'Recall': 0}, {'Class': 61, 'Precision': 0, 'Recall': 0.0}, {'Class': 62, 'Precision': 0, 'Recall': 0}, {'Class': 63, 'Precision': 0, 'Recall': 0}, {'Class': 64, 'Precision': 0, 'Recall': 0.0}, {'Class': 66, 'Precision': 0, 'Recall': 0.0}, {'Class': 67, 'Precision': 0, 'Recall': 0}, {'Class': 68, 'Precision': 0, 'Recall': 0.0}, {'Class': 69, 'Precision': 0, 'Recall': 0}, {'Class': 70, 'Precision': 0, 'Recall': 0.0}, {'Class': 71, 'Precision': 0, 'Recall': 0.0}, {'Class': 72, 'Precision': 0, 'Recall': 0.0}, {'Class': 73, 'Precision': 0, 'Recall': 0.0}, {'Class': 74, 'Precision': 0, 'Recall': 0}, {'Class': 75, 'Precision': 0, 'Recall': 0}, {'Class': 76, 'Precision': 0, 'Recall': 0.0}, {'Class': 77, 'Precision': 0, 'Recall': 0.0}, {'Class': 78, 'Precision': 0, 'Recall': 0}, {'Class': 79, 'Precision': 0, 'Recall': 0.0}, {'Class': 80, 'Precision': 0, 'Recall': 0.0}, {'Class': 81, 'Precision': 0, 'Recall': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Reset model weights to previous state\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=num_labels\n",
    ")\n",
    "\n",
    "def compute_class_weights(dataset: Dataset, num_labels: int, min_count=0, top_k=None):\n",
    "    \"\"\"\n",
    "    Compute class weights for\n",
    "    Args:\n",
    "        dataset (Dataset): A dataset object containing examples with a \"label\" field.\n",
    "        num_labels (int): The number of classes in the dataset.\n",
    "        min_count (int): The minimum count of a class to be considered for computing class weights.\n",
    "    Returns:\n",
    "        dict: A dictionary with class weights for each class compared to the class with maximum count.\n",
    "    \"\"\"\n",
    "    class_counts = Counter(dataset[\"label\"])\n",
    "\n",
    "    if top_k is not None:\n",
    "        class_counts = dict(class_counts.most_common(top_k))\n",
    "\n",
    "    max_count = max(class_counts.values())\n",
    "    class_weights = {label: max_count / count for label, count in class_counts.items()}\n",
    "    class_weights_list = [1.0] * (num_labels)\n",
    "\n",
    "    for label, weight in class_weights.items():\n",
    "        if class_counts[label] > min_count:\n",
    "            class_weights_list[label] = weight\n",
    "            \n",
    "    return class_weights_list\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weights(train_dataset, num_labels=num_labels, top_k=10)\n",
    "for i, weight in enumerate(class_weights):\n",
    "    if weight > 1.0:\n",
    "        print(f\"Class {i} weight: {weight}\")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Define custom compute_loss_func\n",
    "def compute_loss_func(outputs, labels, num_items_in_batch=None):\n",
    "    logits = outputs.logits\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    return loss\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    compute_loss_func=compute_loss_func,\n",
    ")\n",
    "\n",
    "training_args.num_train_epochs = 10\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred, y_true = get_predictions_and_labels(encoded_test_dataset, trainer)\n",
    "compute_precision_recall_per_class(y_pred, y_true, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sqlalchemy>=1.4.2\n",
      "  Downloading SQLAlchemy-2.0.36-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: numpy in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from optuna) (2.0.2)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tqdm in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from optuna) (6.0.2)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Mako\n",
      "  Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/mcelikik/Library/Python/3.9/lib/python/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Installing collected packages: sqlalchemy, Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0 sqlalchemy-2.0.36\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "zsh:1: no matches found: ray[tune]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! pip install optuna\n",
    "! pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 565, Test Size: 189, Val Size: 189\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Split the dataset into train, validation and test sets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from local json file \n",
    "dataset = load_dataset(\"json\", data_files=\"./data/parsed/5thelement.json\")\n",
    "\n",
    "# Split the dataset into a simple train, validation and test sets\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.4)\n",
    "\n",
    "train_dataset = train_test_split['train']\n",
    "\n",
    "temp_split_dataset = train_test_split['test'].train_test_split(test_size=0.5)\n",
    "val_dataset = temp_split_dataset['train']\n",
    "test_dataset = temp_split_dataset['test']\n",
    "\n",
    "print(f\"Train Size: {len(train_dataset)}, Test Size: {len(test_dataset)}, Val Size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 7022.66 examples/s]\n",
      "Map: 100%|██████████| 189/189 [00:00<00:00, 8330.78 examples/s]\n",
      "Map: 100%|██████████| 189/189 [00:00<00:00, 8285.85 examples/s]\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2024-11-30 21:54:22,213] A new study created in memory with name: no-name-5662d24d-8aac-454c-971a-0841b2ea97d4\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                 \n",
      " 25%|██▌       | 189/756 [07:07<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4345, 'grad_norm': 4.090267658233643, 'learning_rate': 8.854597805338979e-06, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [07:18<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3045, 'grad_norm': 9.708539009094238, 'learning_rate': 6.482830536051752e-06, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [07:25<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7897, 'grad_norm': 10.829245567321777, 'learning_rate': 4.111063266764526e-06, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [07:33<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6095, 'grad_norm': 12.190399169921875, 'learning_rate': 1.7392959974772993e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      "\n",
      " 25%|██▌       | 189/756 [07:52<02:22,  3.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5981497764587402, 'eval_accuracy': 0.2222222222222222, 'eval_f1': 0.008658008658008658, 'eval_runtime': 7.1919, 'eval_samples_per_second': 26.28, 'eval_steps_per_second': 3.337, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 142/142 [00:57<00:00,  2.49it/s]\n",
      "[I 2024-11-30 21:55:21,377] Trial 0 finished with value: 0.23088023088023088 and parameters: {'learning_rate': 1.0435775984863796e-05, 'num_train_epochs': 1, 'seed': 7, 'per_device_train_batch_size': 4}. Best is trial 0 with value: 0.23088023088023088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 57.0963, 'train_samples_per_second': 9.896, 'train_steps_per_second': 2.487, 'train_loss': 3.969445242008693, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                 \n",
      " 25%|██▌       | 189/756 [08:09<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2952, 'grad_norm': 8.204130172729492, 'learning_rate': 8.606272816685223e-06, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [08:20<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7404, 'grad_norm': 10.060961723327637, 'learning_rate': 6.301021169358824e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\u001b[A                                             \n",
      "\n",
      " 25%|██▌       | 189/756 [08:29<02:22,  3.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.646695613861084, 'eval_accuracy': 0.2222222222222222, 'eval_f1': 0.008658008658008658, 'eval_runtime': 5.448, 'eval_samples_per_second': 34.691, 'eval_steps_per_second': 4.405, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [08:41<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6168, 'grad_norm': 7.278314113616943, 'learning_rate': 3.9957695220324245e-06, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [08:50<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4228, 'grad_norm': 7.428894519805908, 'learning_rate': 1.6905178747060257e-06, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      " 25%|██▌       | 189/756 [09:06<02:22,  3.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4981696605682373, 'eval_accuracy': 0.2222222222222222, 'eval_f1': 0.008658008658008658, 'eval_runtime': 5.6005, 'eval_samples_per_second': 33.747, 'eval_steps_per_second': 4.285, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 142/142 [01:12<00:00,  1.95it/s]\n",
      "[I 2024-11-30 21:56:35,202] Trial 1 finished with value: 0.23088023088023088 and parameters: {'learning_rate': 1.0143107248236155e-05, 'num_train_epochs': 2, 'seed': 19, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 0.23088023088023088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 72.7923, 'train_samples_per_second': 15.524, 'train_steps_per_second': 1.951, 'train_loss': 3.7333725405410982, 'epoch': 2.0}\n",
      "BestRun(run_id='0', objective=0.23088023088023088, hyperparameters={'learning_rate': 1.0435775984863796e-05, 'num_train_epochs': 1, 'seed': 7, 'per_device_train_batch_size': 4}, run_summary=None)\n"
     ]
    }
   ],
   "source": [
    "# Run hyperparameter search\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "train_dataset_oversampled = oversample_dataset(train_dataset, class_count_threshold=2)\n",
    "\n",
    "encoded_train_dataset = train_dataset_oversampled.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "encoded_val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "trainer_hp = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "best_run = trainer_hp.hyperparameter_search(n_trials=2, direction=\"maximize\")\n",
    "\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [09:27<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0689, 'grad_norm': 11.085065841674805, 'learning_rate': 8.88268156424581e-06, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [09:36<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7027, 'grad_norm': 10.091877937316895, 'learning_rate': 7.206703910614526e-06, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [09:46<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4752, 'grad_norm': 9.614151954650879, 'learning_rate': 5.530726256983241e-06, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [09:56<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5799, 'grad_norm': 9.672201156616211, 'learning_rate': 3.854748603351956e-06, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [10:06<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.525, 'grad_norm': 9.818577766418457, 'learning_rate': 2.1787709497206706e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 189/756 [10:19<02:22,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.392, 'grad_norm': 11.35380744934082, 'learning_rate': 5.027932960893855e-07, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      "\n",
      " 25%|██▌       | 189/756 [10:34<02:22,  3.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.304694652557373, 'eval_accuracy': 0.2804232804232804, 'eval_f1': 0.010186430905246973, 'eval_runtime': 8.0175, 'eval_samples_per_second': 23.573, 'eval_steps_per_second': 2.993, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 189/189 [01:28<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 88.6759, 'train_samples_per_second': 8.503, 'train_steps_per_second': 2.131, 'train_loss': 3.6040518523524048, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:06<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results on Test Set: {'eval_loss': 3.1965441703796387, 'eval_accuracy': 0.328042328042328, 'eval_f1': 0.01073965009527109, 'eval_runtime': 6.1758, 'eval_samples_per_second': 30.603, 'eval_steps_per_second': 3.886, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# predictions in test set: 189\n",
      "Predictions on test set:\n",
      "[41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      " 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41]\n",
      "True labels:\n",
      "[66 21 43 41 41 59 43 45 54 41 44 61 21 41 74 82 41 41 34 21  7 21 82 61\n",
      " 41 41 41 59 55 41 21 26 82 41 41 77  8 41 48 41 41 78 41 66 44 41 55 80\n",
      " 44 41  3 21 41 21 41 41 69 41 41 20 44 41 41 21 72 21 41 44 45 41 21 44\n",
      " 55 41 72 41  1 44 44 82 66 43 41 31 44 41 22 55 41 68 22 72 41 41 21 82\n",
      " 41 43 77 55 59 50 34 29 41 61 68 21 12 43 10 17 41 41 82 41 21 43 66  3\n",
      " 69 21 41 68 82 41 80 73 41 41 72 46 59 65  6 79 69 21 41 41 59  8 71 70\n",
      " 41 57 11  8 21 41 21 43 44 41 41 54 82 12 41 59 29 41 15 21  2 41 21 41\n",
      " 59 41 59 72 61 24 41 41 41 41 43 54 27 79 41 45 41 21 41 41 59]\n",
      "[{'Class': 41, 'Precision': 0.328042328042328, 'Recall': 1.0}, {'Class': 0, 'Precision': 0, 'Recall': 0}, {'Class': 1, 'Precision': 0, 'Recall': 0.0}, {'Class': 2, 'Precision': 0, 'Recall': 0.0}, {'Class': 3, 'Precision': 0, 'Recall': 0.0}, {'Class': 4, 'Precision': 0, 'Recall': 0}, {'Class': 5, 'Precision': 0, 'Recall': 0}, {'Class': 6, 'Precision': 0, 'Recall': 0.0}, {'Class': 7, 'Precision': 0, 'Recall': 0.0}, {'Class': 8, 'Precision': 0, 'Recall': 0.0}, {'Class': 9, 'Precision': 0, 'Recall': 0}, {'Class': 10, 'Precision': 0, 'Recall': 0.0}, {'Class': 11, 'Precision': 0, 'Recall': 0.0}, {'Class': 12, 'Precision': 0, 'Recall': 0.0}, {'Class': 13, 'Precision': 0, 'Recall': 0}, {'Class': 14, 'Precision': 0, 'Recall': 0}, {'Class': 15, 'Precision': 0, 'Recall': 0.0}, {'Class': 16, 'Precision': 0, 'Recall': 0}, {'Class': 17, 'Precision': 0, 'Recall': 0.0}, {'Class': 18, 'Precision': 0, 'Recall': 0}, {'Class': 19, 'Precision': 0, 'Recall': 0}, {'Class': 20, 'Precision': 0, 'Recall': 0.0}, {'Class': 21, 'Precision': 0, 'Recall': 0.0}, {'Class': 22, 'Precision': 0, 'Recall': 0.0}, {'Class': 23, 'Precision': 0, 'Recall': 0}, {'Class': 24, 'Precision': 0, 'Recall': 0.0}, {'Class': 25, 'Precision': 0, 'Recall': 0}, {'Class': 26, 'Precision': 0, 'Recall': 0.0}, {'Class': 27, 'Precision': 0, 'Recall': 0.0}, {'Class': 28, 'Precision': 0, 'Recall': 0}, {'Class': 29, 'Precision': 0, 'Recall': 0.0}, {'Class': 30, 'Precision': 0, 'Recall': 0}, {'Class': 31, 'Precision': 0, 'Recall': 0.0}, {'Class': 32, 'Precision': 0, 'Recall': 0}, {'Class': 33, 'Precision': 0, 'Recall': 0}, {'Class': 34, 'Precision': 0, 'Recall': 0.0}, {'Class': 35, 'Precision': 0, 'Recall': 0}, {'Class': 36, 'Precision': 0, 'Recall': 0}, {'Class': 37, 'Precision': 0, 'Recall': 0}, {'Class': 38, 'Precision': 0, 'Recall': 0}, {'Class': 39, 'Precision': 0, 'Recall': 0}, {'Class': 40, 'Precision': 0, 'Recall': 0}, {'Class': 42, 'Precision': 0, 'Recall': 0}, {'Class': 43, 'Precision': 0, 'Recall': 0.0}, {'Class': 44, 'Precision': 0, 'Recall': 0.0}, {'Class': 45, 'Precision': 0, 'Recall': 0.0}, {'Class': 46, 'Precision': 0, 'Recall': 0.0}, {'Class': 47, 'Precision': 0, 'Recall': 0}, {'Class': 48, 'Precision': 0, 'Recall': 0.0}, {'Class': 49, 'Precision': 0, 'Recall': 0}, {'Class': 50, 'Precision': 0, 'Recall': 0.0}, {'Class': 51, 'Precision': 0, 'Recall': 0}, {'Class': 52, 'Precision': 0, 'Recall': 0}, {'Class': 53, 'Precision': 0, 'Recall': 0}, {'Class': 54, 'Precision': 0, 'Recall': 0.0}, {'Class': 55, 'Precision': 0, 'Recall': 0.0}, {'Class': 56, 'Precision': 0, 'Recall': 0}, {'Class': 57, 'Precision': 0, 'Recall': 0.0}, {'Class': 58, 'Precision': 0, 'Recall': 0}, {'Class': 59, 'Precision': 0, 'Recall': 0.0}, {'Class': 60, 'Precision': 0, 'Recall': 0}, {'Class': 61, 'Precision': 0, 'Recall': 0.0}, {'Class': 62, 'Precision': 0, 'Recall': 0}, {'Class': 63, 'Precision': 0, 'Recall': 0}, {'Class': 64, 'Precision': 0, 'Recall': 0}, {'Class': 65, 'Precision': 0, 'Recall': 0.0}, {'Class': 66, 'Precision': 0, 'Recall': 0.0}, {'Class': 67, 'Precision': 0, 'Recall': 0}, {'Class': 68, 'Precision': 0, 'Recall': 0.0}, {'Class': 69, 'Precision': 0, 'Recall': 0.0}, {'Class': 70, 'Precision': 0, 'Recall': 0.0}, {'Class': 71, 'Precision': 0, 'Recall': 0.0}, {'Class': 72, 'Precision': 0, 'Recall': 0.0}, {'Class': 73, 'Precision': 0, 'Recall': 0.0}, {'Class': 74, 'Precision': 0, 'Recall': 0.0}, {'Class': 75, 'Precision': 0, 'Recall': 0}, {'Class': 76, 'Precision': 0, 'Recall': 0}, {'Class': 77, 'Precision': 0, 'Recall': 0.0}, {'Class': 78, 'Precision': 0, 'Recall': 0.0}, {'Class': 79, 'Precision': 0, 'Recall': 0.0}, {'Class': 80, 'Precision': 0, 'Recall': 0.0}, {'Class': 81, 'Precision': 0, 'Recall': 0}, {'Class': 82, 'Precision': 0, 'Recall': 0.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model with the best hyperparameters\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer_hp.args, n, v)\n",
    "\n",
    "trainer_hp.train()\n",
    "\n",
    "# Evaluate the model\n",
    "### Evaluate the model\n",
    "results = trainer_hp.evaluate(eval_dataset=encoded_test_dataset)\n",
    "print(\"Evaluation Results on Test Set:\", results)\n",
    "\n",
    "y_pred, y_true = get_predictions_and_labels(encoded_test_dataset, trainer)\n",
    "\n",
    "compute_precision_recall_per_class(y_pred, y_true, num_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
